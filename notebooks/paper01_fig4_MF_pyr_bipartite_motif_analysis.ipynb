{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62f69960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cloudvolume import CloudVolume as cv\n",
    "from caveclient import CAVEclient\n",
    "from scipy.spatial import KDTree\n",
    "import pygsheets\n",
    "import ast\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import pickle\n",
    "from scipy.spatial.distance import cdist\n",
    "import networkx as nx\n",
    "import random\n",
    "from itertools import combinations\n",
    "import igraph as ig\n",
    "\n",
    "vol_graphene =cv('graphene://https://minnie.microns-daf.com/segmentation/table/zheng_ca3', agglomerate=True, use_https=True)\n",
    "#vol = cv('gs://zheng_mouse_hippocampus_production/v2/seg_m195',parallel=True, progress=False, use_https=True)\n",
    "client = CAVEclient('zheng_ca3')\n",
    "#t195 = client.materialize.get_timestamp(195)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a6ce1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MF, PYR IDs of timestamp 195\n",
    "\n",
    "gc = pygsheets.authorize(service_file = \"./hippca3-8126bea0d603.json\")\n",
    "sheet = gc.open('CA3_cells')\n",
    "mf_sheet = sheet.worksheet('title','mossy_fibers')\n",
    "pyr_sheet = sheet.worksheet('title','pyramidal_cells')\n",
    "mf_df = mf_sheet.get_as_df()\n",
    "mf_df = mf_df[mf_df['type'] == 'mossy fiber']\n",
    "mf_df = mf_df[mf_df['segid_195'] != '']\n",
    "pc_df = pyr_sheet.get_as_df()\n",
    "\n",
    "sheet2 = gc.open('all_pyramidal_cells')\n",
    "mfpc_sheet = sheet2.worksheet('title','MF-pyr')\n",
    "mfpc_df = mfpc_sheet.get_as_df(start='A4')\n",
    "thorny_nuc_ids = mfpc_df['segid (Nuclei table segID)'].values\n",
    "thorny_df = pc_df[pc_df['nucleus_segids'].isin(thorny_nuc_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f651a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shuffle_adjmat_config_model(adj_matrix):\n",
    "    n_sinks, n_sources = adj_matrix.shape\n",
    "    deg_sources = adj_matrix.sum(axis=0).astype(int).tolist()  # Source (columns)\n",
    "    deg_sinks = adj_matrix.sum(axis=1).astype(int).tolist()    # Sink (rows)\n",
    "    source_stubs = [i for i, deg in enumerate(deg_sources) for _ in range(deg)]\n",
    "    sink_stubs = [i for i, deg in enumerate(deg_sinks) for _ in range(deg)]\n",
    "\n",
    "    if len(source_stubs) != len(sink_stubs):\n",
    "        raise ValueError(\"The total number of stubs for sources and sinks must match.\")\n",
    "    np.random.shuffle(source_stubs)\n",
    "    np.random.shuffle(sink_stubs)\n",
    "    sink_stubs = [s + n_sources for s in sink_stubs]\n",
    "\n",
    "    edges = list(zip(source_stubs, sink_stubs))\n",
    "    types = [0]*n_sources + [1]*n_sinks  # 0=source, 1=sink\n",
    "    # Create bipartite graph\n",
    "    g = ig.Graph(edges=edges, directed=False)\n",
    "    g.vs[\"type\"] = types\n",
    "    # Reconstruct randomized adjacency matrix\n",
    "    rand_adj = np.zeros((n_sinks, n_sources), dtype=int)\n",
    "    for u, v in g.get_edgelist():\n",
    "        if types[u] == 0:\n",
    "            source = u\n",
    "            sink = v - n_sources\n",
    "        else:\n",
    "            source = v\n",
    "            sink = u - n_sources\n",
    "        rand_adj[sink, source] = 1\n",
    "    return rand_adj\n",
    "\n",
    "def configuration_model_from_adjacency(adj):\n",
    "    n_sinks, n_sources = adj.shape\n",
    "\n",
    "    # Step 1: Degree sequences\n",
    "    sink_indegree = adj.sum(axis=1)\n",
    "    source_outdegree = adj.sum(axis=0)\n",
    "\n",
    "    # Step 2: Create stubs\n",
    "    sink_stubs = np.repeat(np.arange(n_sinks), sink_indegree)\n",
    "    source_stubs = np.repeat(np.arange(n_sources), source_outdegree)\n",
    "\n",
    "    # Check matching is possible\n",
    "    assert len(sink_stubs) == len(source_stubs), \"Total in-degree and out-degree must match\"\n",
    "\n",
    "    # Step 3: Shuffle and pair stubs\n",
    "    np.random.shuffle(sink_stubs)\n",
    "    np.random.shuffle(source_stubs)\n",
    "\n",
    "    # Step 4: Build new adjacency matrix\n",
    "    new_adj = np.zeros((n_sinks, n_sources), dtype=int)\n",
    "    for sink, source in zip(sink_stubs, source_stubs):\n",
    "        new_adj[sink, source] += 1  # allows multiedges!\n",
    "\n",
    "    return new_adj\n",
    "\n",
    "def simple_configuration_model(adj, max_attempts=1000):\n",
    "    n_sinks, n_sources = adj.shape\n",
    "    sink_indegree = adj.sum(axis=1)\n",
    "    source_outdegree = adj.sum(axis=0)\n",
    "\n",
    "    # Step 1: Create stubs\n",
    "    sink_stubs = list(np.repeat(np.arange(n_sinks), sink_indegree))\n",
    "    source_stubs = list(np.repeat(np.arange(n_sources), source_outdegree))\n",
    "\n",
    "    if len(sink_stubs) != len(source_stubs):\n",
    "        raise ValueError(\"Total in-degree and out-degree must match\")\n",
    "\n",
    "    # Step 2: Shuffle and try to match without duplicates\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        edges = set()\n",
    "        used_pairs = set()\n",
    "        sink_pool = sink_stubs.copy()\n",
    "        source_pool = source_stubs.copy()\n",
    "        random.shuffle(sink_pool)\n",
    "        random.shuffle(source_pool)\n",
    "\n",
    "        valid = True\n",
    "        for sink, source in zip(sink_pool, source_pool):\n",
    "            if (sink, source) in edges:\n",
    "                valid = False\n",
    "                break\n",
    "            edges.add((sink, source))\n",
    "\n",
    "        if valid:\n",
    "            # Build adjacency matrix\n",
    "            new_adj = np.zeros_like(adj, dtype=int)\n",
    "            for sink, source in edges:\n",
    "                new_adj[sink, source] = 1\n",
    "            return new_adj\n",
    "\n",
    "        attempt += 1\n",
    "\n",
    "    raise RuntimeError(f\"Failed to generate a simple configuration model after {max_attempts} attempts\")\n",
    "    \n",
    "def shuffle_adjmat_radius_model(d_mat, r, seed, coord_df):\n",
    "    d_mat_thresh = (d_mat < r).astype(int)\n",
    "    bouton_indices, thorn_indices = np.where(d_mat_thresh == 1)\n",
    "    edges = [(f\"b{i}\", f\"t{j}\") for i, j in zip(bouton_indices, thorn_indices)]\n",
    "    counters = []\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(edges)\n",
    "    bouton_matched = set()\n",
    "    thorn_matched = set()\n",
    "    bouton_to_thorn = {f\"b{i}\": [] for i in range(d_mat_thresh.shape[0])}\n",
    "\n",
    "    for b, t in edges:\n",
    "        if b not in bouton_matched and t not in thorn_matched:\n",
    "            bouton_to_thorn[b].append(t)\n",
    "            bouton_matched.add(b)\n",
    "            thorn_matched.add(t)\n",
    "\n",
    "    for b, t in edges:\n",
    "        if b not in bouton_matched:\n",
    "            bouton_to_thorn[b].append(t)\n",
    "            bouton_matched.add(b)\n",
    "\n",
    "    thorn_to_bouton = defaultdict(list)\n",
    "    for k, vals in bouton_to_thorn.items():\n",
    "        for v in vals:\n",
    "            thorn_to_bouton[v].append(k)\n",
    "\n",
    "    target_mf_ids = coord_df['pre_ids'].unique()\n",
    "    target_pyr_ids = coord_df['post_ids'].unique()\n",
    "    adj_mat_rand = np.zeros((len(target_pyr_ids), len(target_mf_ids)), dtype=int)\n",
    "    mf_to_pyr_randomized = {i: [] for i in range(len(target_mf_ids))}\n",
    "    for key, val in bouton_to_thorn.items():\n",
    "        key = int(key.lstrip('b'))\n",
    "        val = int(val[0].lstrip('t'))\n",
    "        this_mf_id = coord_df['pre_ids'].values[key]\n",
    "        this_pyr_id = coord_df['post_ids'].values[val]\n",
    "        idx_mf = np.where(target_mf_ids==this_mf_id)[0][0]\n",
    "        idx_pyr = np.where(target_pyr_ids==this_pyr_id)[0][0]\n",
    "        mf_to_pyr_randomized[idx_mf].append(idx_pyr)\n",
    "\n",
    "    for key, vals in mf_to_pyr_randomized.items():\n",
    "        for val in vals:\n",
    "            adj_mat_rand[val,key] = 1\n",
    "            \n",
    "    return adj_mat_rand\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f211ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e17bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously generated adjmatrix, mf_ids, pc_ids, dist_mat\n",
    "syn_thresh = 3\n",
    "with open('./variables/adjmat_and_bouton_thorn_dist_matrix_250707_th' +str(syn_thresh)+'.pkl','rb') as f:\n",
    "    adj_mat_weighted, mf_ids, pc_ids, coord_df, d_mat, syn_df = pickle.load(f)\n",
    "    \n",
    "adj_mat = (adj_mat_weighted != 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4514825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pyr IDs used:  630\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "1730.6373629570007\n"
     ]
    }
   ],
   "source": [
    "# Construct an adjacency matrix between Pyr and MF\n",
    "# Use flat segmentation m195\n",
    "\n",
    "'''\n",
    "mf_ids_temp = mf_df['segid_195'].values\n",
    "mf_ids_temp = [ids for ids in mf_ids_temp if ids != '']\n",
    "pc_ids = thorny_df['segid_195'].values\n",
    "syn_thresh = 3\n",
    "\n",
    "print('Number of pyr IDs used: ', len(pc_ids))\n",
    "\n",
    "mf_manually_identifed_zhihao_pyr_648518346442090245 = [648518346369197640, 648518346383876745, 648518346383931017, 648518346394388476, 648518346404290697, 648518346409172934, 648518346421440163, 648518346426154487, 648518346426411414, 648518346426420886, 648518346428546878, 648518346428550718, 648518346428663779, 648518346430163710, 648518346430885722, 648518346431036176, 648518346431151376, 648518346431549859, 648518346431600214, 648518346431791040, 648518346432844718, 648518346433020736, 648518346433096829, 648518346433221277, 648518346433736019, 648518346433921124, 648518346434062966, 648518346434614442, 648518346435060257, 648518346435163644, 648518346435244540, 648518346435414392, 648518346435440760, 648518346435754623, 648518346435847544, 648518346435859832, 648518346436065028, 648518346436078200, 648518346436228728, 648518346436369272, 648518346436382072, 648518346436401175, 648518346436693531, 648518346436705323, 648518346436722199, 648518346436736888, 648518346436772728, 648518346436812827, 648518346436846810, 648518346436863879, 648518346436864218, 648518346436879301, 648518346436940408, 648518346436940920, 648518346436943736, 648518346436945016, 648518346436991709, 648518346437144005, 648518346437153034, 648518346437220984, 648518346437606030, 648518346437646346, 648518346437703982, 648518346437708516, 648518346437794346, 648518346437952042, 648518346438002630, 648518346438005190, 648518346438014406, 648518346438015360, 648518346438040127, 648518346438203078, 648518346438206120, 648518346438206406, 648518346438251718, 648518346438313791, 648518346438453275, 648518346438515188, 648518346438518079, 648518346438561855, 648518346438640582, 648518346438653812, 648518346438684358, 648518346438687551, 648518346438764276, 648518346438783551, 648518346438820415, 648518346438825791, 648518346438878650, 648518346438901702, 648518346438906431, 648518346438960959, 648518346438964799, 648518346439152447, 648518346439198018, 648518346439207836, 648518346439213631, 648518346439263295, 648518346439277631, 648518346439281343, 648518346439342143, 648518346439351871, 648518346439383103, 648518346439389503, 648518346439405887, 648518346439407935, 648518346439419455, 648518346439428759, 648518346439468695, 648518346439582600, 648518346439608924, 648518346439682623, 648518346440669455, 648518346440679183, 648518346440681118, 648518346440710823, 648518346440934253, 648518346441454294, 648518346441492694, 648518346441526230, 648518346441841035, 648518346441946571, 648518346442016351, 648518346442032162, 648518346442147557, 648518346442163595, 648518346442226225, 648518346442336816, 648518346442421808, 648518346442445063, 648518346442493685, 648518346442643759, 648518346442651401, 648518346442704687, 648518346442751880, 648518346442853076, 648518346442855124, 648518346443143728, 648518346443797823, 648518346444000183, 648518346444116601, 648518346444121921, 648518346444141377, 648518346444158585, 648518346444184385, 648518346444248129, 648518346444422375, 648518346444427239, 648518346444899791, 648518346444900493, 648518346444935939, 648518346445025283, 648518346445155891, 648518346445178675, 648518346445187649, 648518346445190962, 648518346445220673, 648518346445225750, 648518346445231169, 648518346445248065, 648518346445274689, 648518346445295497, 648518346445441417, 648518346445443191, 648518346445477149, 648518346445514269, 648518346445550386, 648518346445580082, 648518346445582130, 648518346445585714, 648518346445635011, 648518346445680442, 648518346445725379, 648518346445897522, 648518346446288590, 648518346446493481, 648518346446582888, 648518346446707241, 648518346446976065, 648518346446976970, 648518346447037057, 648518346447156339, 648518346447310642, 648518346447354418, 648518346447393489, 648518346447455361, 648518346447456385, 648518346447474305, 648518346447525697, 648518346447615873, 648518346447711347, 648518346447793980, 648518346447828851, 648518346447832947, 648518346447833459, 648518346447835251, 648518346447837756, 648518346447993148, 648518346447993515, 648518346447996843, 648518346448016243, 648518346448115059, 648518346448142951, 648518346448152167, 648518346448266411, 648518346448345297, 648518346448817011, 648518346448999027, 648518346449141633, 648518346449212801, 648518346449263397, 648518346449535556, 648518346449619663, 648518346450711887, 648518346450867219, 648518346450876769, 648518346451037583, 648518346451056545, 648518346451302387, 648518346451320379, 648518346451377651, 648518346451686971, 648518346451879663, 648518346451948781, 648518346451960301, 648518346452209350, 648518346452277222, 648518346452496102, 648518346454342988, 648518346455185289, 648518346456752565, 648518346457259378, 648518346457442418, 648518346457511861, 648518346457513909, 648518346457514421, 648518346457565714, 648518346457607605, 648518346457831605, 648518346457896885, 648518346458130819, 648518346458391427, 648518346459186051, 648518346461813379, 648518346462388867, 648518346462747023, 648518346462753935, 648518346463194474, 648518346463266959, 648518346465033615, 648518346465295601, 648518346465707407, 648518346465858798, 648518346466597617, 648518346476441254, 648518346476549030, 648518346478250918]\n",
    "mf_to_add = np.array([val for i, val in enumerate(mf_manually_identifed_zhihao_pyr_648518346442090245) if val not in mf_ids_temp])\n",
    "mf_ids = list(np.append(mf_ids_temp,mf_to_add))\n",
    "\n",
    "pre_ids = []\n",
    "post_ids = []\n",
    "adj_mat = np.zeros((len(pc_ids), len(mf_ids)), dtype=int)\n",
    "\n",
    "stime = time.time()\n",
    "for i in range(0, len(pc_ids)):\n",
    "    syn = client.materialize.synapse_query(pre_ids=mf_ids, post_ids=pc_ids[i], bounding_box=None, bounding_box_column='post_pt_position', \n",
    "                timestamp=t195, remove_autapses=True, include_zeros=False, limit=None, offset=None, \n",
    "                split_positions=False, desired_resolution=[1000,1000,1000], materialization_version=None, \n",
    "                synapse_table='synapses_ca3_v1', datastack_name='zheng_ca3', metadata=True) \n",
    "    syn2 = syn.groupby('pre_pt_root_id')\n",
    "    syn_grouped = {name: group for name, group in syn2 if len(group)>=syn_thresh}\n",
    "\n",
    "    for key, group in syn_grouped.items():\n",
    "        pre_ids.append(key)\n",
    "        post_ids.append(pc_ids[i])\n",
    "        j = mf_ids.index(key)\n",
    "        adj_mat[i,j] = 1\n",
    "        \n",
    "    if i%100 ==0:\n",
    "        print(i)\n",
    "etime = time.time()\n",
    "print(etime - stime)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f181632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-node fan-in: 1030653\n",
      "3-node fan-out: 5216\n",
      "4-node fan-in: 38234155\n",
      "4-node fan-out: 630\n",
      "4-node butterfly: 505\n",
      "path-of-length-3: 10432\n"
     ]
    }
   ],
   "source": [
    "# 3-node, 4-node motif analyses of bipartite graph\n",
    "\n",
    "def count_bipartite_motifs(adj_matrix):\n",
    "    n_sinks, n_sources = adj_matrix.shape\n",
    "\n",
    "    fanin3 = 0\n",
    "    fanout3 = 0\n",
    "    fanin4 = 0\n",
    "    fanout4 = 0\n",
    "    butterfly = 0\n",
    "    path3 = 0\n",
    "    \n",
    "    # Fan-in\n",
    "    for sink in range(n_sinks):\n",
    "        sources = np.where(adj_matrix[sink])[0]\n",
    "        if len(sources) >= 2:\n",
    "            fanin3 += len(list(combinations(sources, 2)))\n",
    "        if len(sources) >= 3:\n",
    "            fanin4 += len(list(combinations(sources, 3)))\n",
    "\n",
    "    # Fan-out\n",
    "    for source in range(n_sources):\n",
    "        sinks = np.where(adj_matrix[:, source])[0]\n",
    "        if len(sinks) >= 2:\n",
    "            fanout3 += len(list(combinations(sinks, 2)))\n",
    "        if len(sinks) >= 3:\n",
    "            fanout4 += len(list(combinations(sinks, 3)))\n",
    "\n",
    "    # Butterfly (slow exhaustive count)\n",
    "    '''\n",
    "    for src1, src2 in combinations(range(n_sources), 2):\n",
    "        common_sinks = np.where(adj_matrix[:, src1] & adj_matrix[:, src2])[0]\n",
    "        if len(common_sinks) >= 2:\n",
    "            butterfly += len(list(combinations(common_sinks, 2)))\n",
    "    '''\n",
    "    \n",
    "    # Butterfly: optimized strategy\n",
    "    for s1, s2 in combinations(range(n_sinks), 2):\n",
    "        common_sources = np.where(adj_matrix[s1] & adj_matrix[s2])[0]\n",
    "        k = len(common_sources)\n",
    "        if k >= 2:\n",
    "            butterfly += k * (k - 1) // 2 \n",
    "            \n",
    "    # Path of length 3: (A–B–A–B)\n",
    "    for sink1 in range(n_sinks):\n",
    "        src1s = np.where(adj_matrix[sink1])[0]\n",
    "        for src in src1s:\n",
    "            sink2s = np.where(adj_matrix[:, src])[0]\n",
    "            for sink2 in sink2s:\n",
    "                if sink2 != sink1:\n",
    "                    path3 += 1\n",
    "                    \n",
    "    sharing = 1 / (1 + (path3 / (butterfly*4)))\n",
    "\n",
    "    return {\n",
    "        \"3-node fan-in\": fanin3,\n",
    "        \"3-node fan-out\": fanout3,\n",
    "        \"4-node fan-in\": fanin4,\n",
    "        \"4-node fan-out\": fanout4,\n",
    "        \"4-node butterfly\": butterfly,\n",
    "        \"path-of-length-3\": path3,\n",
    "    }\n",
    "\n",
    "motifs = count_bipartite_motifs(adj_mat)\n",
    "for key, val in motifs.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eead5cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.719329833984375e-05\n",
      "466\n",
      "466\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "stime = time.time()\n",
    "aa = sum([math.comb(i,2)*j for i,j in {2: 233, 3: 25, 4: 9, 5: 4, 6: 1, 7: 1, 8: 1}.items()])\n",
    "etime = time.time()\n",
    "print(etime-stime)\n",
    "print(aa)\n",
    "print(butterfly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c6ac1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1/100 random models.\n",
      "  Processed 11/100 random models.\n",
      "  Processed 21/100 random models.\n",
      "  Processed 31/100 random models.\n",
      "  Processed 41/100 random models.\n",
      "  Processed 51/100 random models.\n",
      "  Processed 61/100 random models.\n",
      "  Processed 71/100 random models.\n",
      "  Processed 81/100 random models.\n",
      "  Processed 91/100 random models.\n",
      "3-node fan-in: mean = 1029345.58, std = 450.02\n",
      "3-node fan-out: mean = 5200.04, std = 5.56\n",
      "4-node fan-in: mean = 38131198.00, std = 45586.99\n",
      "4-node fan-out: mean = 625.07, std = 4.02\n",
      "4-node butterfly: mean = 131.01, std = 13.67\n",
      "path-of-length-3: mean = 10400.08, std = 11.11\n"
     ]
    }
   ],
   "source": [
    "# Count bipartite motifs on configuration models\n",
    "\n",
    "N=100\n",
    "\n",
    "clustering_coeff = []\n",
    "motif_records = defaultdict(list)\n",
    "for ii in range(N):\n",
    "    \n",
    "    adj_rand = shuffle_adjmat_config_model(adj_mat)\n",
    "    #adj_rand = simple_configuration_model(adj_mat, max_attempts=10000)\n",
    "    # check degree-preservation\n",
    "    #print(\"Preserved sink in-degrees %:\", np.sum(adj_mat.sum(axis=1) == adj_rand.sum(axis=1))/ adj_mat.shape[0] *100)\n",
    "    #print(\"Preserved source out-degrees %:\",np.sum(adj_mat.sum(axis=0) == adj_rand.sum(axis=0))/ adj_mat.shape[1] *100)\n",
    "    \n",
    "    counts = count_bipartite_motifs(adj_rand)\n",
    "    for motif, count in counts.items():\n",
    "        motif_records[motif].append(count)\n",
    "    \n",
    "    if ii % 10 == 0:\n",
    "        print(f\"  Processed {ii+1}/{N} random models.\")\n",
    "        \n",
    "stats = {\n",
    "    motif: {\n",
    "        \"mean\": np.mean(vals),\n",
    "        \"std\": np.std(vals)\n",
    "    }\n",
    "    for motif, vals in motif_records.items()\n",
    "}\n",
    "\n",
    "for motif, s in stats.items():\n",
    "    print(f\"{motif}: mean = {s['mean']:.2f}, std = {s['std']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01a2845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1/100 random models.\n",
      "  Processed 11/100 random models.\n",
      "  Processed 21/100 random models.\n",
      "  Processed 31/100 random models.\n",
      "  Processed 41/100 random models.\n",
      "  Processed 51/100 random models.\n",
      "  Processed 61/100 random models.\n",
      "  Processed 71/100 random models.\n",
      "  Processed 81/100 random models.\n",
      "  Processed 91/100 random models.\n",
      "3-node fan-in: mean = 1019388.71, std = 2109.27\n",
      "3-node fan-out: mean = 4992.31, std = 14.82\n",
      "4-node fan-in: mean = 37514752.94, std = 224652.81\n",
      "4-node fan-out: mean = 581.50, std = 7.15\n",
      "4-node butterfly: mean = 443.27, std = 25.35\n",
      "path-of-length-3: mean = 9984.62, std = 29.64\n"
     ]
    }
   ],
   "source": [
    "# Count bipartite motifs on radius models\n",
    "\n",
    "N=100\n",
    "r=10\n",
    "clustering_coeff = []\n",
    "motif_records = defaultdict(list)\n",
    "for ii in range(N):\n",
    "    adj_rand = shuffle_adjmat_radius_model(d_mat,r,ii, coord_df)\n",
    "    counts = count_bipartite_motifs(adj_rand)\n",
    "    for motif, count in counts.items():\n",
    "        motif_records[motif].append(count)\n",
    "    if ii % 10 == 0:\n",
    "        print(f\"  Processed {ii+1}/{N} random models.\")\n",
    "        \n",
    "stats = {\n",
    "    motif: {\n",
    "        \"mean\": np.mean(vals),\n",
    "        \"std\": np.std(vals)\n",
    "    }\n",
    "    for motif, vals in motif_records.items()\n",
    "}\n",
    "\n",
    "for motif, s in stats.items():\n",
    "    print(f\"{motif}: mean = {s['mean']:.2f}, std = {s['std']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 um radius\n",
    "3-node fan-in: mean = 1185051.74, std = 1471.14\n",
    "3-node fan-out: mean = 3468.33, std = 6.21\n",
    "4-node fan-in: mean = 48590262.84, std = 159524.17\n",
    "4-node fan-out: mean = 193.98, std = 1.92\n",
    "4-node butterfly: mean = 256.73, std = 16.82\n",
    "path-of-length-3: mean = 6936.66, std = 12.42\n",
    "clustering coeff: mean = 0.13, std = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81db0763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380\n",
      "87153\n",
      "0.1042212697727483\n"
     ]
    }
   ],
   "source": [
    "# (Projected graph onto Pyr) Using igraph: Draw an edge between pyr cells if they share a common MF and count motifs\n",
    "import igraph as ig \n",
    "\n",
    "pc_indices, mf_indices = np.where(adj_mat == 1)\n",
    "edges = [(f\"m{i}\", f\"p{j}\") for i, j in zip(mf_indices, pc_indices)]\n",
    "\n",
    "source_to_sinks = defaultdict(list)\n",
    "for src, sink, in edges:\n",
    "    source_to_sinks[src].append(sink)\n",
    "\n",
    "# Project onto sinks (connect sinks that share a source)\n",
    "projected_edges = set()\n",
    "for sinks in source_to_sinks.values():\n",
    "    for i in range(len(sinks)):\n",
    "        for j in range(i + 1, len(sinks)):\n",
    "            projected_edges.add(tuple(sorted((sinks[i], sinks[j]))))\n",
    "\n",
    "sink_nodes = list(set(sink for _, sink in edges))\n",
    "g_sink = ig.Graph()\n",
    "g_sink.add_vertices(sink_nodes)\n",
    "g_sink.add_edges(list(projected_edges))\n",
    "\n",
    "# Count motifs and clustering\n",
    "triangle_count_observed = g_sink.motifs_randesu(size=3)[3]  # triangle = motif ID 3\n",
    "open_triplet_count_observed = g_sink.motifs_randesu(size=3)[2]  # open triplet = motif ID 2\n",
    "clustering_coeff_observed = g_sink.transitivity_undirected()\n",
    "\n",
    "\n",
    "print(triangle_count_observed)\n",
    "print(open_triplet_count_observed)\n",
    "print(clustering_coeff_observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a791b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.303202174169916\n",
      "0.12043546321332005\n"
     ]
    }
   ],
   "source": [
    "a = (1+(61807/(2821*3)))\n",
    "print(1/a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38726ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1/100 random models.\n",
      "  Processed 11/100 random models.\n",
      "  Processed 21/100 random models.\n",
      "  Processed 31/100 random models.\n",
      "  Processed 41/100 random models.\n",
      "  Processed 51/100 random models.\n",
      "  Processed 61/100 random models.\n",
      "  Processed 71/100 random models.\n",
      "  Processed 81/100 random models.\n",
      "  Processed 91/100 random models.\n",
      "3-node motifs on a projected graph onto pyr cells\n",
      "------------------------------\n",
      "triangle_count_observed =  3380\n",
      "open_triplet_count_observed =  87153\n",
      "clustering_coeff_observed =  0.1042212697727483\n",
      "**************************************************\n",
      "\n",
      "Stats of 3-node motifs on  100  projected random configuration models\n",
      "------------------------------\n",
      "Mean(triangle) =  2299.1\n",
      "StdDev(triangle) =  77.23632565056418\n",
      "------------------------------\n",
      "Mean(open_triplet) =  105343.63\n",
      "StdDev(open_triplet) =  1282.5190887858162\n",
      "------------------------------\n",
      "Mean(clustering coeff) =  0.061441105042971456\n",
      "StdDev(clustering coeff) =  0.0015548980212102602\n"
     ]
    }
   ],
   "source": [
    "# (Projected graph onto Pyr) Using igraph: Count motifs on projected configuration models\n",
    "\n",
    "N=100\n",
    "\n",
    "counts_triangle = []\n",
    "counts_open_triplet = []\n",
    "clustering_coeff = []\n",
    "\n",
    "for ii in range(N):\n",
    "    adj_rand = shuffle_adjmat_config_model(adj_mat)\n",
    "    pc_indices, mf_indices = np.where(adj_rand == 1)\n",
    "    edges = [(f\"m{i}\", f\"p{j}\") for i, j in zip(mf_indices, pc_indices)]\n",
    "\n",
    "    source_to_sinks = defaultdict(list)\n",
    "    for src, sink, in edges:\n",
    "        source_to_sinks[src].append(sink)\n",
    "\n",
    "    projected_edges = set()\n",
    "    for sinks in source_to_sinks.values():\n",
    "        for i in range(len(sinks)):\n",
    "            for j in range(i + 1, len(sinks)):\n",
    "                projected_edges.add(tuple(sorted((sinks[i], sinks[j]))))\n",
    "    sink_nodes = list(set(sink for _, sink in edges))\n",
    "    g_sink = ig.Graph()\n",
    "    g_sink.add_vertices(sink_nodes)\n",
    "    g_sink.add_edges(list(projected_edges))\n",
    "    triangle_count = g_sink.motifs_randesu(size=3)[3]  # triangle = motif ID 3\n",
    "    open_triplet_count = g_sink.motifs_randesu(size=3)[2]  # open triplet = motif ID 2\n",
    "    clustering = g_sink.transitivity_undirected()\n",
    "       \n",
    "    counts_triangle.append(triangle_count)\n",
    "    counts_open_triplet.append(open_triplet_count)\n",
    "    clustering_coeff.append(clustering)\n",
    "    \n",
    "    if ii % 10 == 0:\n",
    "        print(f\"  Processed {ii+1}/{N} random models.\")\n",
    "        \n",
    "        \n",
    "print(\"3-node motifs on a projected graph onto pyr cells\")\n",
    "print(\"-\"*30)\n",
    "print(\"triangle_count_observed = \" ,triangle_count_observed)\n",
    "print(\"open_triplet_count_observed = \", open_triplet_count_observed)\n",
    "print(\"clustering_coeff_observed = \", clustering_coeff_observed)\n",
    "print(\"*\"*50)\n",
    "print()\n",
    "print(\"Stats of 3-node motifs on \", str(N), \" projected random configuration models\")\n",
    "print(\"-\"*30)\n",
    "print(\"Mean(triangle) = \", np.mean(counts_triangle))\n",
    "print(\"StdDev(triangle) = \", np.std(counts_triangle))\n",
    "print(\"-\"*30)\n",
    "print(\"Mean(open_triplet) = \", np.mean(counts_open_triplet))\n",
    "print(\"StdDev(open_triplet) = \", np.std(counts_open_triplet))\n",
    "print(\"-\"*30)\n",
    "print(\"Mean(clustering coeff) = \", np.mean(clustering_coeff))\n",
    "print(\"StdDev(clustering coeff) = \", np.std(clustering_coeff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a99bfc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1/100 random models.\n",
      "  Processed 11/100 random models.\n",
      "  Processed 21/100 random models.\n",
      "  Processed 31/100 random models.\n",
      "  Processed 41/100 random models.\n",
      "  Processed 51/100 random models.\n",
      "  Processed 61/100 random models.\n",
      "  Processed 71/100 random models.\n",
      "  Processed 81/100 random models.\n",
      "  Processed 91/100 random models.\n",
      "Stats of 3-node motifs on  100  projected radius< 10  models\n",
      "------------------------------\n",
      "Mean(triangle) =  2497.71\n",
      "StdDev(triangle) =  73.3508411676376\n",
      "------------------------------\n",
      "Mean(open_triplet) =  57855.78\n",
      "StdDev(open_triplet) =  642.5964764920518\n",
      "------------------------------\n",
      "Mean(clustering coeff) =  0.11464907062606362\n",
      "StdDev(clustering coeff) =  0.00246806409818598\n"
     ]
    }
   ],
   "source": [
    "# (Projected graph onto Pyr cells) Using igraph (faster): Count motifs on projected radius models\n",
    "\n",
    "N=100\n",
    "r=10\n",
    "counts_triangle2 = []\n",
    "counts_open_triplet2 = []\n",
    "clustering_coeff2 = []\n",
    "\n",
    "for ii in range(N):\n",
    "    \n",
    "    adj_rand = shuffle_adjmat_radius_model(d_mat,r,ii, coord_df)\n",
    "    pc_indices, mf_indices = np.where(adj_rand == 1)\n",
    "    edges = [(f\"m{i}\", f\"p{j}\") for i, j in zip(mf_indices, pc_indices)]\n",
    "\n",
    "    source_to_sinks = defaultdict(list)\n",
    "    for src, sink, in edges:\n",
    "        source_to_sinks[src].append(sink)\n",
    "\n",
    "    projected_edges = set()\n",
    "    for sinks in source_to_sinks.values():\n",
    "        for i in range(len(sinks)):\n",
    "            for j in range(i + 1, len(sinks)):\n",
    "                projected_edges.add(tuple(sorted((sinks[i], sinks[j]))))\n",
    "    sink_nodes = list(set(sink for _, sink in edges))\n",
    "    g_sink = ig.Graph()\n",
    "    g_sink.add_vertices(sink_nodes)\n",
    "    g_sink.add_edges(list(projected_edges))\n",
    "    triangle_count = g_sink.motifs_randesu(size=3)[3]  # triangle = motif ID 3\n",
    "    open_triplet_count = g_sink.motifs_randesu(size=3)[2]  # open triplet = motif ID 2\n",
    "    clustering = g_sink.transitivity_undirected()\n",
    "       \n",
    "    counts_triangle2.append(triangle_count)\n",
    "    counts_open_triplet2.append(open_triplet_count)\n",
    "    clustering_coeff2.append(clustering)\n",
    "    \n",
    "    if ii % 10 == 0:\n",
    "        print(f\"  Processed {ii+1}/{N} random models.\")\n",
    "        \n",
    "\n",
    "print(\"Stats of 3-node motifs on \", str(N), \" projected radius<\", str(r),\" models\")\n",
    "print(\"-\"*30)\n",
    "print(\"Mean(triangle) = \", np.mean(counts_triangle2))\n",
    "print(\"StdDev(triangle) = \", np.std(counts_triangle2))\n",
    "print(\"-\"*30)\n",
    "print(\"Mean(open_triplet) = \", np.mean(counts_open_triplet2))\n",
    "print(\"StdDev(open_triplet) = \", np.std(counts_open_triplet2))\n",
    "print(\"-\"*30)\n",
    "print(\"Mean(clustering coeff) = \", np.mean(clustering_coeff2))\n",
    "print(\"StdDev(clustering coeff) = \", np.std(clustering_coeff2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "179d783b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1/100 random models.\n",
      "  Processed 11/100 random models.\n",
      "  Processed 21/100 random models.\n",
      "  Processed 31/100 random models.\n",
      "  Processed 41/100 random models.\n",
      "  Processed 51/100 random models.\n",
      "  Processed 61/100 random models.\n",
      "  Processed 71/100 random models.\n",
      "  Processed 81/100 random models.\n",
      "  Processed 91/100 random models.\n",
      "Stats of 3-node motifs on  100  projected radius< 50  models\n",
      "------------------------------\n",
      "Mean(triangle) =  2326.19\n",
      "StdDev(triangle) =  72.35878592126875\n",
      "------------------------------\n",
      "Mean(open_triplet) =  63068.55\n",
      "StdDev(open_triplet) =  683.0718025947199\n",
      "------------------------------\n",
      "Mean(clustering coeff) =  0.09961358817151028\n",
      "StdDev(clustering coeff) =  0.002363489599021539\n"
     ]
    }
   ],
   "source": [
    "# (Projected graph onto Pyr cells) Using igraph (faster): Count motifs on projected radius models\n",
    "\n",
    "N=100\n",
    "r=50\n",
    "counts_triangle2 = []\n",
    "counts_open_triplet2 = []\n",
    "clustering_coeff2 = []\n",
    "\n",
    "for ii in range(N):\n",
    "    adj_rand = shuffle_adjmat_radius_model(d_mat,r,ii, coord_df)\n",
    "    pc_indices, mf_indices = np.where(adj_rand == 1)\n",
    "    edges = [(f\"m{i}\", f\"p{j}\") for i, j in zip(mf_indices, pc_indices)]\n",
    "\n",
    "    source_to_sinks = defaultdict(list)\n",
    "    for src, sink, in edges:\n",
    "        source_to_sinks[src].append(sink)\n",
    "\n",
    "    projected_edges = set()\n",
    "    for sinks in source_to_sinks.values():\n",
    "        for i in range(len(sinks)):\n",
    "            for j in range(i + 1, len(sinks)):\n",
    "                projected_edges.add(tuple(sorted((sinks[i], sinks[j]))))\n",
    "    sink_nodes = list(set(sink for _, sink in edges))\n",
    "    g_sink = ig.Graph()\n",
    "    g_sink.add_vertices(sink_nodes)\n",
    "    g_sink.add_edges(list(projected_edges))\n",
    "    triangle_count = g_sink.motifs_randesu(size=3)[3]  # triangle = motif ID 3\n",
    "    open_triplet_count = g_sink.motifs_randesu(size=3)[2]  # open triplet = motif ID 2\n",
    "    clustering = g_sink.transitivity_undirected()\n",
    "       \n",
    "    counts_triangle2.append(triangle_count)\n",
    "    counts_open_triplet2.append(open_triplet_count)\n",
    "    clustering_coeff2.append(clustering)\n",
    "    \n",
    "    if ii % 10 == 0:\n",
    "        print(f\"  Processed {ii+1}/{N} random models.\")\n",
    "        \n",
    "\n",
    "print(\"Stats of 3-node motifs on \", str(N), \" projected radius<\", str(r),\" models\")\n",
    "print(\"-\"*30)\n",
    "print(\"Mean(triangle) = \", np.mean(counts_triangle2))\n",
    "print(\"StdDev(triangle) = \", np.std(counts_triangle2))\n",
    "print(\"-\"*30)\n",
    "print(\"Mean(open_triplet) = \", np.mean(counts_open_triplet2))\n",
    "print(\"StdDev(open_triplet) = \", np.std(counts_open_triplet2))\n",
    "print(\"-\"*30)\n",
    "print(\"Mean(clustering coeff) = \", np.mean(clustering_coeff2))\n",
    "print(\"StdDev(clustering coeff) = \", np.std(clustering_coeff2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cde76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8473ebbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting 3-node motifs...\n",
      "counting 4-node motifs...\n"
     ]
    }
   ],
   "source": [
    "# Create a bipartite graph between MF (source) and pyr cells (sink)\n",
    "# No cycles are possible\n",
    "# 3-Node Motifs: [(MF1,MF2) -> Pyr1]     [MF1 -> (Pyr1,Pyr2)]\n",
    "# 4-Node Motifs: [(MF1,MF2) -> (Pyr1,Pyr2)]     [MF1 -> (Pyr1,Pyr2) and MF2 -> Pyr1]   \n",
    "\n",
    "\n",
    "pc_indices, mf_indices = np.where(adj_mat == 1)\n",
    "edges = [(f\"m{i}\", f\"p{j}\") for i, j in zip(mf_indices, pc_indices)]\n",
    "mf_nodes = [f\"m{i}\" for i in range(len(mf_ids))]\n",
    "pc_nodes = [f\"p{j}\" for j in range(len(pc_ids))]\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(mf_nodes, bipartite = 0)\n",
    "G.add_nodes_from(pc_nodes, bipartite = 1)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "nodes_to_remove = []\n",
    "for node in mf_nodes:\n",
    "    if G.degree(node) == 0:\n",
    "        nodes_to_remove.append(node)\n",
    "G.remove_nodes_from(nodes_to_remove)\n",
    "mf_nodes = [n for n,d in G.nodes(data=True) if d['bipartite']==0]\n",
    "\n",
    "# 3-node motifs\n",
    "print('counting 3-node motifs...')\n",
    "motif_counts = Counter()\n",
    "for pc in pc_nodes:\n",
    "    neighbors = list(G.neighbors(pc))\n",
    "    for mf1, mf2 in combinations(neighbors, 2):\n",
    "        motif_counts['fan_in_3_node'] += 1\n",
    "\n",
    "for mf in mf_nodes:\n",
    "    neighbors = list(G.neighbors(mf))\n",
    "    for pc1, pc2 in combinations(neighbors, 2):\n",
    "        motif_counts['fan_out_3_node'] += 1\n",
    "        \n",
    "# 4-node motifs\n",
    "print('counting 4-node motifs...')\n",
    "for mf1, mf2 in combinations(mf_nodes, 2):\n",
    "    pyr_common = set(G.neighbors(mf1)) & set(G.neighbors(mf2))\n",
    "    if len(pyr_common) >= 2:\n",
    "        motif_counts['butterfly_4_node'] += (len(pyr_common) * (len(pyr_common)-1) // 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "980d8cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939757\n",
      "4043\n",
      "901\n"
     ]
    }
   ],
   "source": [
    "print(motif_counts['fan_in_3_node'])\n",
    "print(motif_counts['fan_out_3_node'])\n",
    "print(motif_counts['butterfly_4_node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2512e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1/100 random models.\n",
      "  Processed 2/100 random models.\n",
      "  Processed 3/100 random models.\n",
      "  Processed 4/100 random models.\n",
      "  Processed 5/100 random models.\n",
      "  Processed 6/100 random models.\n",
      "  Processed 7/100 random models.\n",
      "  Processed 8/100 random models.\n",
      "  Processed 9/100 random models.\n",
      "  Processed 10/100 random models.\n",
      "  Processed 11/100 random models.\n",
      "  Processed 12/100 random models.\n",
      "  Processed 13/100 random models.\n",
      "  Processed 14/100 random models.\n",
      "  Processed 15/100 random models.\n",
      "  Processed 16/100 random models.\n",
      "  Processed 17/100 random models.\n",
      "  Processed 18/100 random models.\n",
      "  Processed 19/100 random models.\n",
      "  Processed 20/100 random models.\n",
      "  Processed 21/100 random models.\n",
      "  Processed 22/100 random models.\n",
      "  Processed 23/100 random models.\n",
      "  Processed 24/100 random models.\n",
      "  Processed 25/100 random models.\n",
      "  Processed 26/100 random models.\n",
      "  Processed 27/100 random models.\n",
      "  Processed 28/100 random models.\n",
      "  Processed 29/100 random models.\n",
      "  Processed 30/100 random models.\n",
      "  Processed 31/100 random models.\n",
      "  Processed 32/100 random models.\n",
      "  Processed 33/100 random models.\n",
      "  Processed 34/100 random models.\n",
      "  Processed 35/100 random models.\n",
      "  Processed 36/100 random models.\n",
      "  Processed 37/100 random models.\n",
      "  Processed 38/100 random models.\n",
      "  Processed 39/100 random models.\n",
      "  Processed 40/100 random models.\n",
      "  Processed 41/100 random models.\n",
      "  Processed 42/100 random models.\n",
      "  Processed 43/100 random models.\n",
      "  Processed 44/100 random models.\n",
      "  Processed 45/100 random models.\n",
      "  Processed 46/100 random models.\n",
      "  Processed 47/100 random models.\n",
      "  Processed 48/100 random models.\n",
      "  Processed 49/100 random models.\n",
      "  Processed 50/100 random models.\n",
      "  Processed 51/100 random models.\n",
      "  Processed 52/100 random models.\n",
      "  Processed 53/100 random models.\n",
      "  Processed 54/100 random models.\n",
      "  Processed 55/100 random models.\n",
      "  Processed 56/100 random models.\n",
      "  Processed 57/100 random models.\n",
      "  Processed 58/100 random models.\n",
      "  Processed 59/100 random models.\n",
      "  Processed 60/100 random models.\n",
      "  Processed 61/100 random models.\n",
      "  Processed 62/100 random models.\n",
      "  Processed 63/100 random models.\n",
      "  Processed 64/100 random models.\n",
      "  Processed 65/100 random models.\n",
      "  Processed 66/100 random models.\n",
      "  Processed 67/100 random models.\n",
      "  Processed 68/100 random models.\n",
      "  Processed 69/100 random models.\n",
      "  Processed 70/100 random models.\n",
      "  Processed 71/100 random models.\n",
      "  Processed 72/100 random models.\n",
      "  Processed 73/100 random models.\n",
      "  Processed 74/100 random models.\n",
      "  Processed 75/100 random models.\n",
      "  Processed 76/100 random models.\n",
      "  Processed 77/100 random models.\n",
      "  Processed 78/100 random models.\n",
      "  Processed 79/100 random models.\n",
      "  Processed 80/100 random models.\n",
      "  Processed 81/100 random models.\n",
      "  Processed 82/100 random models.\n",
      "  Processed 83/100 random models.\n",
      "  Processed 84/100 random models.\n",
      "  Processed 85/100 random models.\n",
      "  Processed 86/100 random models.\n",
      "  Processed 87/100 random models.\n",
      "  Processed 88/100 random models.\n",
      "  Processed 89/100 random models.\n",
      "  Processed 90/100 random models.\n",
      "  Processed 91/100 random models.\n",
      "  Processed 92/100 random models.\n",
      "  Processed 93/100 random models.\n",
      "  Processed 94/100 random models.\n",
      "  Processed 95/100 random models.\n",
      "  Processed 96/100 random models.\n",
      "  Processed 97/100 random models.\n",
      "  Processed 98/100 random models.\n",
      "  Processed 99/100 random models.\n",
      "  Processed 100/100 random models.\n"
     ]
    }
   ],
   "source": [
    "# Create a random null models \n",
    "\n",
    "N=100\n",
    "motif_counts_null = Counter()\n",
    "motif_counts_null['fan_in_3_node'] = []\n",
    "motif_counts_null['fan_out_3_node'] = []\n",
    "motif_counts_null['butterfly_4_node'] = []\n",
    "mf_degrees = {node: G.degree(node) for node in mf_nodes}\n",
    "pc_degrees = {node: G.degree(node) for node in pc_nodes}\n",
    "for ii in range(N):\n",
    "    G_rand = nx.bipartite.configuration_model([mf_degrees[n] for n in mf_nodes], [pc_degrees[n] for n in pc_nodes], seed=ii)\n",
    "    G_rand = nx.Graph(G_rand)\n",
    "    for node in mf_nodes:\n",
    "        if node in G_rand: G_rand.nodes[node]['bipartite'] = 0\n",
    "    for node in pc_nodes:\n",
    "        if node in G_rand: G_rand.nodes[node]['bipartite'] = 1\n",
    "            \n",
    "    top_nodes = [n for n, d in G_rand.nodes(data=True) if d['bipartite']==0]\n",
    "    bottom_nodes = [n for n ,d in G_rand.nodes(data=True) if d['bipartite']==1]\n",
    "\n",
    "    # 3-node motifs\n",
    "    counts_fan_in = 0\n",
    "    counts_fan_out = 0\n",
    "    counts_butterfly = 0\n",
    "    for pc in bottom_nodes:\n",
    "        neighbors = list(G_rand.neighbors(pc))\n",
    "        for mf1, mf2 in combinations(neighbors, 2):\n",
    "            counts_fan_in += 1\n",
    "\n",
    "    for mf in top_nodes:\n",
    "        neighbors = list(G_rand.neighbors(mf))\n",
    "        for pc1, pc2 in combinations(neighbors, 2):\n",
    "            counts_fan_out += 1\n",
    "\n",
    "    # 4-node motifs\n",
    "    for mf1, mf2 in combinations(top_nodes, 2):\n",
    "        pyr_common = set(G_rand.neighbors(mf1)) & set(G_rand.neighbors(mf2))\n",
    "        if len(pyr_common) >= 2:\n",
    "            counts_butterfly += (len(pyr_common) * (len(pyr_common)-1) // 2)\n",
    "            \n",
    "    motif_counts_null['fan_in_3_node'].append(counts_fan_in)\n",
    "    motif_counts_null['fan_out_3_node'].append(counts_fan_out)\n",
    "    motif_counts_null['butterfly_4_node'].append(counts_butterfly)\n",
    "    print(f\"  Processed {ii+1}/{N} random models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7504ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'fan_in_3_node': 939757, 'fan_out_3_node': 4043, 'butterfly_4_node': 901})\n",
      "Counter({'fan_in_3_node': [937937, 938424, 939181, 938409, 938674, 938075, 938418, 938208, 938285, 938990], 'fan_out_3_node': [4022, 4027, 4036, 4029, 4030, 4021, 4020, 4020, 4025, 4030], 'butterfly_4_node': [143, 139, 121, 140, 126, 115, 117, 110, 128, 133]})\n"
     ]
    }
   ],
   "source": [
    "print(motif_counts)\n",
    "print(motif_counts_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad5e38ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motif analysis stats from observed data\n",
      "fan_in_3_node        Counts: 939757    \n",
      "fan_out_3_node       Counts: 4043      \n",
      "butterfly_4_node     Counts: 901       \n",
      "----------------------------------------------------------\n",
      "Motif analysis stats from 100 random configuration models\n",
      "fan_in_3_node        Mean: 938648.62  Std Dev: 387.03\n",
      "fan_out_3_node       Mean: 4028.18    Std Dev: 4.98\n",
      "butterfly_4_node     Mean: 124.07     Std Dev: 11.51\n"
     ]
    }
   ],
   "source": [
    "print(f'Motif analysis stats from observed data')\n",
    "for motif, counts_list in motif_counts.items():\n",
    "    print(f\"{motif:<20} Counts: {counts_list:<10}\")    \n",
    "print('----------------------------------------------------------')\n",
    "print(f'Motif analysis stats from {N} random configuration models')\n",
    "for motif, counts_list in motif_counts_null.items():\n",
    "    mean_count = np.mean(np.array(counts_list))\n",
    "    std_dev = np.std(np.array(counts_list))\n",
    "    print(f\"{motif:<20} Mean: {mean_count:<10} Std Dev: {std_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33f4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22fcdb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279.27818417549133\n"
     ]
    }
   ],
   "source": [
    "# (TOO SLOW) Using Networkx: Draw an edge between pyr cells if they share a common MF\n",
    "\n",
    "def classify_3_node_motif(subg):\n",
    "    degrees = sorted(dict(subg.degree()).values())\n",
    "    if degrees == [1, 1, 2]:\n",
    "        return 'Open Triplet'\n",
    "    elif degrees == [2, 2, 2]:\n",
    "        return 'Triangle'\n",
    "    return 'Other'\n",
    "\n",
    "def count_and_classify_motifs(G, size):\n",
    "    motif_counts = Counter()\n",
    "    for nodes in combinations(G.nodes, size):\n",
    "        subg = G.subgraph(nodes)\n",
    "        if nx.is_connected(subg):\n",
    "            if size == 3:\n",
    "                motif = classify_3_node_motif(subg)\n",
    "            elif size == 4:\n",
    "                motif = classify_4_node_motif(subg)\n",
    "            else:\n",
    "                motif = 'Unknown'\n",
    "            motif_counts[motif] += 1\n",
    "    return motif_counts\n",
    "\n",
    "pc_indices, mf_indices = np.where(adj_mat == 1)\n",
    "edges = [(f\"m{i}\", f\"p{j}\") for i, j in zip(mf_indices, pc_indices)]\n",
    "mf_nodes = [f\"m{i}\" for i in range(len(mf_ids))]\n",
    "pc_nodes = [f\"p{j}\" for j in range(len(pc_ids))]\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(mf_nodes, bipartite = 0)\n",
    "G.add_nodes_from(pc_nodes, bipartite = 1)\n",
    "G.add_edges_from(edges)\n",
    "nodes_to_remove = []\n",
    "for node in mf_nodes:\n",
    "    if G.degree(node) == 0:\n",
    "        nodes_to_remove.append(node)\n",
    "G.remove_nodes_from(nodes_to_remove)\n",
    "mf_nodes = [n for n,d in G.nodes(data=True) if d['bipartite']==0]\n",
    "\n",
    "P_edges = []\n",
    "for pc1, pc2 in combinations(pc_nodes,2):\n",
    "    mf_common = set(G.neighbors(pc1)) & set(G.neighbors(pc2))\n",
    "    if len(mf_common) > 0:\n",
    "        P_edges.append((pc1,pc2,len(mf_common)))\n",
    "        \n",
    "P = nx.Graph()\n",
    "for u,v,w in P_edges:\n",
    "    P.add_edge(u,v, weight=w)\n",
    "    \n",
    "stime = time.time()\n",
    "motifs_3 = count_and_classify_motifs(P, 3)\n",
    "etime = time.time()\n",
    "print(etime -stime)\n",
    "\n",
    "#motifs_4 = count_and_classify_motifs(P, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa0ce31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00012993812561035156\n",
      "Counter({'Triangle': 4})\n",
      "[[0 1 1 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [1 0 0 1 1]\n",
      " [1 1 0 0 0]]\n",
      "[('p0', 'p2', 2), ('p0', 'p3', 1), ('p0', 'p4', 1), ('p2', 'p3', 1), ('p2', 'p4', 1), ('p3', 'p4', 1)]\n",
      "[[0 1 1 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 1]\n",
      " [1 1 0 1 0]\n",
      " [1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# test the motif count code (networkx)\n",
    "\n",
    "n_sink = 5\n",
    "n_source = 5\n",
    "edge_prob = 0.5\n",
    "rng = np.random.default_rng(0)\n",
    "adj = (rng.random((n_sink, n_source)) < edge_prob).astype(int)\n",
    "\n",
    "pc_indices, mf_indices = np.where(adj == 1)\n",
    "edges = [(f\"m{i}\", f\"p{j}\") for i, j in zip(mf_indices, pc_indices)]\n",
    "mf_nodes = [f\"m{i}\" for i in range(len(mf_ids))]\n",
    "pc_nodes = [f\"p{j}\" for j in range(len(pc_ids))]\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(mf_nodes, bipartite = 0)\n",
    "G.add_nodes_from(pc_nodes, bipartite = 1)\n",
    "G.add_edges_from(edges)\n",
    "nodes_to_remove = []\n",
    "for node in mf_nodes:\n",
    "    if G.degree(node) == 0:\n",
    "        nodes_to_remove.append(node)\n",
    "G.remove_nodes_from(nodes_to_remove)\n",
    "mf_nodes = [n for n,d in G.nodes(data=True) if d['bipartite']==0]\n",
    "\n",
    "P_edges = []\n",
    "for pc1, pc2 in combinations(pc_nodes,2):\n",
    "    mf_common = set(G.neighbors(pc1)) & set(G.neighbors(pc2))\n",
    "    if len(mf_common) > 0:\n",
    "        P_edges.append((pc1,pc2,len(mf_common)))\n",
    "        \n",
    "P = nx.Graph()\n",
    "for u,v,w in P_edges:\n",
    "    P.add_edge(u,v, weight=w)\n",
    "    \n",
    "stime = time.time()\n",
    "motifs_3 = count_and_classify_motifs(P, 3)\n",
    "etime = time.time()\n",
    "print(etime -stime)\n",
    "print(motifs_3)\n",
    "print(adj)\n",
    "print(P_edges)\n",
    "\n",
    "randadj = shuffle_adjmat_config_model(adj)\n",
    "print(randadj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
