{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "801bc405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.spatial import ConvexHull\n",
    "import ast\n",
    "from itertools import chain\n",
    "\n",
    "#Skeleton\n",
    "import pcg_skel\n",
    "\n",
    "# CloudVolume and Cave setup\n",
    "from cloudvolume import CloudVolume\n",
    "from caveclient import CAVEclient\n",
    "sv = CloudVolume('graphene://https://minnie.microns-daf.com/segmentation/table/zheng_ca3', use_https=True, lru_bytes=int(1e8))\n",
    "client = CAVEclient('zheng_ca3')\n",
    "auth = client.auth\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7041a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_segids_df(df, super_voxel_col):\n",
    "    # Get the current date\n",
    "    current_date = datetime.now().strftime('%Y%m%d')  # Format: YYYYMMDD\n",
    "    \n",
    "    # Update segids\n",
    "    updated_segid_list = client.chunkedgraph.get_roots(df[super_voxel_col])\n",
    "    \n",
    "    # Add the updated segids to the DataFrame with the date in the column name\n",
    "    updated_col_name = f\"updated_segids_{current_date}\"\n",
    "    df[updated_col_name] = updated_segid_list\n",
    "    \n",
    "    print(f\"Number of updated segids: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_xyz_metrics(data):\n",
    "    \"\"\"\n",
    "    Compute pairwise distance variance and mean for a single or nested list of 3D points.\n",
    "\n",
    "    Parameters:\n",
    "        data (list): A list or nested list of 3D point lists.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the variance and mean of pairwise distances.\n",
    "    \"\"\"\n",
    "    if not data or not isinstance(data, list):\n",
    "        var = 0\n",
    "        dis = 0\n",
    "        return var, dis\n",
    "\n",
    "    # Flatten the nested list if necessary\n",
    "    if isinstance(data[0], list) and isinstance(data[0][0], list):  # Nested list\n",
    "        flattened_data = [point for sublist in data for point in sublist]\n",
    "    elif isinstance(data[0], list):  # Single list of points\n",
    "        flattened_data = data\n",
    "    else:\n",
    "        raise ValueError(\"Input data must contain 3D points as lists.\")\n",
    "\n",
    "    # Check if the points have 3 coordinates\n",
    "    for point in flattened_data:\n",
    "        if not isinstance(point, list) or len(point) != 3:\n",
    "            raise ValueError(\"All points must be lists of three numeric coordinates.\")\n",
    "\n",
    "    # Convert to numpy array\n",
    "    points_array = np.array(flattened_data)\n",
    "\n",
    "    # Compute pairwise distances\n",
    "    distances = pdist(points_array, metric='euclidean')\n",
    "    var = np.var(distances)\n",
    "    dis = np.mean(distances)\n",
    "\n",
    "    return var, dis\n",
    "\n",
    "\n",
    "def bouton_detector(dataframe, synapse_threshold, voxel_threshold):\n",
    "    # List to store results for creating the DataFrame\n",
    "    results = []\n",
    "    \n",
    "    # Convert 'pre_pt_position' and 'pre_pt_root_id' to NumPy arrays for faster operations\n",
    "    positions = np.array(list(dataframe['pre_pt_position']))\n",
    "    pre_pt_root_ids = dataframe['pre_pt_root_id'].values\n",
    "    post_pt_root_ids = dataframe['post_pt_root_id'].values\n",
    "    \n",
    "    # Iterate over each unique 'pre_pt_root_id'\n",
    "    unique_ids = np.unique(pre_pt_root_ids)\n",
    "    \n",
    "    for unique_id in unique_ids:\n",
    "        # Filter the dataframe for the current 'pre_pt_root_id'\n",
    "        temp_df = dataframe[dataframe['pre_pt_root_id'] == unique_id]\n",
    "        \n",
    "        # Extract the positions and corresponding post IDs\n",
    "        temp_positions = np.array(list(temp_df['pre_pt_position']))\n",
    "        temp_post_ids = temp_df['post_pt_root_id'].values\n",
    "        \n",
    "        # Initialize a bouton counter and lists for bouton positions and bouton partners\n",
    "        bouton_counter = 0\n",
    "        bouton_positions = []\n",
    "        bouton_post_partners = []\n",
    "        visited = np.zeros(len(temp_positions), dtype=bool)\n",
    "        \n",
    "        for i in range(len(temp_positions)):\n",
    "            if visited[i]:\n",
    "                continue\n",
    "            \n",
    "            # Calculate distances from the current position to all others\n",
    "            distances = np.sqrt(np.sum((temp_positions - temp_positions[i]) ** 2, axis=1))\n",
    "            \n",
    "            # Find indices of rows within the voxel threshold\n",
    "            close_points = np.where((distances <= voxel_threshold) & (~visited))[0]\n",
    "            \n",
    "            # If 4 or more rows are within the voxel threshold, count it as a bouton\n",
    "            if len(close_points) >= synapse_threshold:\n",
    "                bouton_counter += 1\n",
    "                visited[close_points] = True\n",
    "                \n",
    "                # Save the positions and unique post IDs of this bouton group\n",
    "                bouton_positions.append(temp_positions[close_points].tolist())\n",
    "                bouton_post_partners.append(list(set(temp_post_ids[close_points])))\n",
    "        \n",
    "        # Combine groups that are within 1000 of each other\n",
    "        combined_positions = []\n",
    "        combined_post_partners = []\n",
    "        \n",
    "        while bouton_positions:\n",
    "            group = bouton_positions.pop(0)\n",
    "            group_post_ids = bouton_post_partners.pop(0)\n",
    "            group = np.array(group)\n",
    "            to_merge = []\n",
    "            \n",
    "            for idx, other_group in enumerate(bouton_positions):\n",
    "                other_group = np.array(other_group)\n",
    "                other_post_ids = bouton_post_partners[idx]\n",
    "                # Check if the current group is within 1000 of another group\n",
    "                distances = np.sqrt(np.sum((group[:, None] - other_group[None, :]) ** 2, axis=2))\n",
    "                #if np.any(distances <= 1000):\n",
    "                if np.any(distances <= 300):\n",
    "                    to_merge.append((other_group, other_post_ids))\n",
    "            \n",
    "            # Merge all nearby groups into the current group\n",
    "            for merge_group, merge_post_ids in to_merge:\n",
    "                group = np.vstack((group, merge_group))\n",
    "                group_post_ids = list(set(group_post_ids + merge_post_ids))\n",
    "                bouton_positions.remove(merge_group.tolist())\n",
    "                bouton_post_partners.remove(merge_post_ids)\n",
    "            \n",
    "            combined_positions.append(group.tolist())\n",
    "            combined_post_partners.append(group_post_ids)\n",
    "        \n",
    "        # Update bouton count after merging\n",
    "        bouton_counter = len(combined_positions)\n",
    "        \n",
    "        # Find bouton volume (area contianed by a single bouton - axon will be massive)\n",
    "        bouton_volume = find_volume(combined_positions)\n",
    "\n",
    "        # Find the variance to eliminate axons, they are evenly disributed. \n",
    "        synapse_variance, synapse_mean_distance = compute_xyz_metrics(combined_positions)\n",
    "        \n",
    "        # Append results to the list\n",
    "        for i in range(bouton_counter):\n",
    "            results.append({\n",
    "                \"pre_pt_root_id\": unique_id,\n",
    "                \"bouton_id\": i + 1,\n",
    "                \"bouton_positions\": combined_positions[i],\n",
    "                \"bouton_partners\": combined_post_partners[i],\n",
    "                \"total_boutons\": bouton_counter,  # Added total bouton count\n",
    "                \"bouton_volume\": bouton_volume,   # Added bouton volume\n",
    "                \"synapse_variance\": synapse_variance, # Added synapse variance\n",
    "                \"synapse_mean_distance\": synapse_mean_distance\n",
    "            })\n",
    "    \n",
    "    # Create a DataFrame from the results\n",
    "    boutons_df = pd.DataFrame(results) \n",
    "\n",
    "    return boutons_df\n",
    "\n",
    "\n",
    "def processor_guts(idx, presyn_segids_chunk, results_list, counter_error, start_time, second_time=False):\n",
    "    global api_call_counter, api_call_start_time  # Use global variables for API call tracking\n",
    "    results = []\n",
    "\n",
    "    # Check API rate limit before making the synapse query\n",
    "    manage_api_rate_limit()\n",
    "\n",
    "    try:\n",
    "        # Query synapse data for the current chunk of 'presyn_segids'\n",
    "        df_synapse = client.materialize.synapse_query(\n",
    "            pre_ids=presyn_segids_chunk,  # Pass the chunk of presyn_segids\n",
    "            post_ids=None,              \n",
    "            synapse_table=\"synapses_ca3_v1\",\n",
    "            desired_resolution=[18, 18, 45]\n",
    "        )\n",
    "        api_call_counter += 1  # Increment API call counter\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying synapse for chunk {presyn_segids_chunk}: {e}\")\n",
    "        print(\"Retrying after 20 seconds...\")\n",
    "        counter_error += 1\n",
    "        print(f\"Number of Errors: {counter_error}\")\n",
    "        time.sleep(20)\n",
    "        manage_api_rate_limit()  # Ensure rate limit before retrying\n",
    "        try:\n",
    "            df_synapse = client.materialize.synapse_query(\n",
    "                pre_ids=presyn_segids_chunk, \n",
    "                post_ids=None,              \n",
    "                synapse_table=\"synapses_ca3_v1\",\n",
    "                desired_resolution=[18, 18, 45]\n",
    "            )\n",
    "            api_call_counter += 1  # Increment API call counter\n",
    "\n",
    "        except Exception as e2:\n",
    "            print(f\"Second attempt failed for chunk {presyn_segids_chunk}: {e2}\")\n",
    "            print(\"Skipping this chunk.\")\n",
    "            return results_list, counter_error, start_time, idx  # Return early if failed\n",
    "\n",
    "    # Process the DataFrame to find boutons\n",
    "    try:\n",
    "        if not df_synapse.empty:\n",
    "            unique_presyn_ids = df_synapse[\"pre_pt_root_id\"].unique()\n",
    "            for presyn_id in unique_presyn_ids:\n",
    "                temp_df = df_synapse[df_synapse[\"pre_pt_root_id\"] == presyn_id]\n",
    "                postsyn_list = temp_df[\"post_pt_root_id\"].unique()\n",
    "                postsyn_num = len(postsyn_list)\n",
    "\n",
    "                if second_time:\n",
    "                    bouton_result = bouton_detector_2(temp_df, 8, 500)\n",
    "                    #bouton_result = bouton_detector_2(temp_df, 10, 1000)\n",
    "\n",
    "                else:\n",
    "                    bouton_result = bouton_detector(temp_df, 8, 500)\n",
    "                    #bouton_result = bouton_detector(temp_df, 10, 1000)\n",
    "\n",
    "\n",
    "                # Iterate over the bouton results and save them\n",
    "                for _, row in bouton_result.iterrows():\n",
    "                    results.append({\n",
    "                        \"presyn_segid\": presyn_id,\n",
    "                        \"postsyn_num\": postsyn_num,\n",
    "                        \"postsyn_id_list\": postsyn_list.tolist(),\n",
    "                        \"pre_pt_root_id\": row['pre_pt_root_id'],\n",
    "                        \"bouton_id\": row['bouton_id'],\n",
    "                        \"bouton_positions\": row['bouton_positions'],\n",
    "                        \"bouton_partners\": row['bouton_partners'],\n",
    "                        \"bouton_volume\": row['bouton_volume'],\n",
    "                        \"synapse_variance\": row['synapse_variance'],\n",
    "                        \"synapse_mean_distance\": row['synapse_mean_distance']\n",
    "                    })\n",
    "        else:\n",
    "            print(\"No synapses in dataframe, dataframe empty.\")\n",
    "    except Exception as bouton_error:\n",
    "        print(f\"Error processing boutons for chunk {presyn_segids_chunk}: {bouton_error}\")\n",
    "\n",
    "    # Append results to the results list\n",
    "    results_list.extend(results)\n",
    "    return results_list, counter_error, start_time, idx\n",
    "\n",
    "\n",
    "def bouton_processor(df, limit_TF=False, limit=5, chunk_size=300):\n",
    "    # List to store results\n",
    "    results_list = []\n",
    "    start_time = time.time()\n",
    "    counter_error = 0\n",
    "\n",
    "    # Process each chunk of 'updated_segids_20241227' column with tqdm progress bar\n",
    "    current_date = datetime.now().strftime('%Y%m%d')  # Format: YYYYMMDD\n",
    "    updated_col_name = f\"updated_segids_{current_date}\" \n",
    "    segids = df[updated_col_name].tolist()\n",
    "    \n",
    "    # Create chunks of presyn_segids\n",
    "    chunks = [segids[i:i + chunk_size] for i in range(0, len(segids), chunk_size)]\n",
    "    \n",
    "    for idx, presyn_segids_chunk in enumerate(tqdm(chunks, desc=\"Processing Mossy Fiber Synapses\")):\n",
    "        # Limit iterations for testing purposes\n",
    "        if limit_TF and idx > limit:\n",
    "            print(f\"Maximum index of {limit} reached. Stopping processing.\")\n",
    "            break\n",
    "\n",
    "        # Process the current chunk of presyn_segids\n",
    "        results_list, counter_error, start_time, error_prevent_count = processor_guts(idx, presyn_segids_chunk, \\\n",
    "                                                results_list, counter_error, start_time, second_time=False)\n",
    "\n",
    "    # Combine all results into a single DataFrame if there are any results\n",
    "    if results_list:\n",
    "        MF_POSTSYN_DF = pd.DataFrame(results_list)\n",
    "    else:\n",
    "        MF_POSTSYN_DF = pd.DataFrame()\n",
    "        \n",
    "    # Count up the number of boutons\n",
    "    MF_POSTSYN_DF['total_boutons'] = MF_POSTSYN_DF.groupby('presyn_segid')['bouton_id'].transform('count')\n",
    "\n",
    "    return MF_POSTSYN_DF\n",
    "\n",
    "\n",
    "def find_bouton_partners(POSTSYN_DATA):  \n",
    "    # Create a new DataFrame with only one row per unique presyn_segid\n",
    "    unique_presyn_df = POSTSYN_DATA.drop_duplicates(subset='presyn_segid', keep='first')\n",
    "    # Initialize a dictionary to store all bouton partners and their corresponding presyn_segids\n",
    "    all_bouton_to_presyn = {}\n",
    "\n",
    "    # Iterate through the DataFrame\n",
    "    for index, row in unique_presyn_df.iterrows():\n",
    "        if isinstance(row['bouton_partners'], str):\n",
    "            try:\n",
    "                bouton_list = eval(row['bouton_partners'])  # Convert string to list\n",
    "            except:\n",
    "                continue\n",
    "        elif isinstance(row['bouton_partners'], list):\n",
    "            bouton_list = row['bouton_partners']\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Map each bouton partner to its corresponding presyn_segid\n",
    "        for bouton in bouton_list:\n",
    "            if bouton not in all_bouton_to_presyn:\n",
    "                all_bouton_to_presyn[bouton] = []\n",
    "            all_bouton_to_presyn[bouton].append(row['presyn_segid'])\n",
    "\n",
    "    # Remove duplicate presyn_segids for each bouton_partner\n",
    "    all_bouton_to_presyn = {key: list(set(value)) for key, value in all_bouton_to_presyn.items()}\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    all_bouton_df = pd.DataFrame({\n",
    "        'bouton_partner': list(all_bouton_to_presyn.keys()),\n",
    "        'presyn_segids': list(all_bouton_to_presyn.values())\n",
    "    })\n",
    "\n",
    "    # Add a column for the number of unique presyn_segids\n",
    "    all_bouton_df['num_presyn_segids'] = all_bouton_df['presyn_segids'].apply(len)\n",
    "    \n",
    "    # Sort the DataFrame by 'num_presyn_segids' in descending order\n",
    "    all_bouton_df_sorted = all_bouton_df.sort_values(by='num_presyn_segids', ascending=False)\n",
    "\n",
    "    # Display the resulting DataFrame\n",
    "    return all_bouton_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7e3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to track API calls within a minute\n",
    "api_call_counter = 0\n",
    "api_call_start_time = time.time()\n",
    "\n",
    "def nonMossy_partner_analysis(df, chunk_size=100):\n",
    "    global api_call_counter, api_call_start_time\n",
    "\n",
    "    # Reset index and initialize columns\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df[\"Potential_Mossy_Partners\"] = None  # Initialize the column with None\n",
    "\n",
    "    # Initialize additional columns for bouton attributes\n",
    "    bouton_columns = [\n",
    "        \"pre_pt_root_id\", \"bouton_id\", \"bouton_positions\", \n",
    "        \"bouton_partners\", \"bouton_volume\", \"synapse_variance\", \n",
    "        \"synapse_mean_distance\"\n",
    "    ]\n",
    "    for col in bouton_columns:\n",
    "        df[col] = None\n",
    "\n",
    "    error_prevent_count = 0\n",
    "\n",
    "    for idx, bouton_partner in enumerate(tqdm(df['bouton_partner'], desc=\"Processing Mossy Fiber Synapses\")):\n",
    "        while True:  # Retry logic\n",
    "            try:\n",
    "                # Handle rate limiting\n",
    "                manage_api_rate_limit()\n",
    "\n",
    "                # Query the synapse data for the current bouton partner\n",
    "                df_synapse_temp = client.materialize.synapse_query(\n",
    "                    pre_ids=None,\n",
    "                    post_ids=[bouton_partner],\n",
    "                    synapse_table=\"synapses_ca3_v1\",\n",
    "                    desired_resolution=[18, 18, 45]\n",
    "                )\n",
    "                api_call_counter += 1\n",
    "\n",
    "                if not df_synapse_temp.empty:\n",
    "                    postsyn_list = df_synapse_temp[\"post_pt_root_id\"].unique()\n",
    "                    postsyn_num = len(postsyn_list)\n",
    "\n",
    "                    # Filter and deduplicate results\n",
    "                    filtered_df = df_synapse_temp[\n",
    "                        df_synapse_temp['pre_pt_root_id'].map(df_synapse_temp['pre_pt_root_id'].value_counts()) > 5\n",
    "                    ]\n",
    "                    filtered_df = filtered_df.drop_duplicates(subset='pre_pt_root_id')\n",
    "\n",
    "                    # Run the bouton detector function with chunking\n",
    "                    bouton_result, error_prevent_count, _ = bouton_processor_2(\n",
    "                        filtered_df, chunk_size=chunk_size, idx=idx, \n",
    "                        error_prevent_count=error_prevent_count, start_time=api_call_start_time\n",
    "                    )\n",
    "\n",
    "                    # If bouton results are found, assign them to the respective columns\n",
    "                    if not bouton_result.empty:\n",
    "                        for col in bouton_columns:\n",
    "                            df.at[idx, col] = bouton_result[col].tolist()\n",
    "\n",
    "                    # Save Potential Mossy Partners\n",
    "                    df.at[idx, 'Potential_Mossy_Partners'] = filtered_df['pre_pt_root_id'].tolist()\n",
    "\n",
    "                break  # Exit the retry loop if successful\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing bouton partner {bouton_partner}: {e}\")\n",
    "                print(\"Retrying after 20 seconds...\")\n",
    "                time.sleep(20)\n",
    "\n",
    "    # Remove rows where 'Potential_Mossy_Partners' is empty\n",
    "    df = df[df['Potential_Mossy_Partners'].apply(lambda x: x is not None and len(x) > 0)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def bouton_processor_2(df, chunk_size=300, limit_TF=False, limit=5, idx=0, error_prevent_count=0, start_time=0):\n",
    "    global api_call_counter, api_call_start_time\n",
    "\n",
    "    # List to store results\n",
    "    results_list = []\n",
    "    counter_error = 0\n",
    "\n",
    "    # Get unique presyn_segids and create chunks\n",
    "    presyn_segids = df['pre_pt_root_id'].unique()\n",
    "    chunks = [presyn_segids[i:i + chunk_size] for i in range(0, len(presyn_segids), chunk_size)]\n",
    "\n",
    "    for idx_2, presyn_segids_chunk in enumerate(chunks):\n",
    "        while True:  # Retry logic\n",
    "            try:\n",
    "                # Handle rate limiting\n",
    "                manage_api_rate_limit()\n",
    "\n",
    "                # Process the current chunk of presyn_segids\n",
    "                results_list, counter_error, start_time, error_prevent_count = processor_guts(\n",
    "                    error_prevent_count, presyn_segids_chunk, results_list, \n",
    "                    counter_error, start_time, second_time=True\n",
    "                )\n",
    "                api_call_counter += 1\n",
    "\n",
    "                break  # Exit the retry loop if successful\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk {presyn_segids_chunk}: {e}\")\n",
    "                print(\"Retrying after 20 seconds...\")\n",
    "                time.sleep(20)\n",
    "\n",
    "    # Combine all results into a single DataFrame if there are any results\n",
    "    if results_list:\n",
    "        MF_POSTSYN_DF = pd.DataFrame(results_list)\n",
    "    else:\n",
    "        MF_POSTSYN_DF = pd.DataFrame()\n",
    "\n",
    "    return MF_POSTSYN_DF, error_prevent_count, start_time\n",
    "\n",
    "\n",
    "def manage_api_rate_limit():\n",
    "    \"\"\"Ensures that the number of API calls does not exceed the limit.\"\"\"\n",
    "    global api_call_counter, api_call_start_time\n",
    "\n",
    "    elapsed_time = time.time() - api_call_start_time\n",
    "    if elapsed_time < 60 and api_call_counter >= 300:\n",
    "        time_to_sleep = 60 - elapsed_time\n",
    "        print(f\"Rate limit reached. Sleeping for {time_to_sleep:.2f} seconds...\")\n",
    "        time.sleep(time_to_sleep)\n",
    "        api_call_start_time = time.time()  # Reset the timer\n",
    "        api_call_counter = 0\n",
    "    elif elapsed_time >= 60:\n",
    "        # Reset counter and timer after a minute\n",
    "        api_call_start_time = time.time()\n",
    "        api_call_counter = 0\n",
    "\n",
    "\n",
    "def bouton_detector_2(dataframe, synapse_threshold, voxel_threshold):\n",
    "    # Dictionary to store results, keyed by `pre_pt_root_id`\n",
    "    results_by_bouton = {}\n",
    "\n",
    "    # Convert 'pre_pt_position' and 'pre_pt_root_id' to NumPy arrays for faster operations\n",
    "    positions = np.array(list(dataframe['pre_pt_position']))\n",
    "    pre_pt_root_ids = dataframe['pre_pt_root_id'].values\n",
    "    post_pt_root_ids = dataframe['post_pt_root_id'].values\n",
    "\n",
    "    # Iterate over each unique 'pre_pt_root_id'\n",
    "    unique_ids = np.unique(pre_pt_root_ids)\n",
    "\n",
    "    for unique_id in unique_ids:\n",
    "        # Filter the dataframe for the current 'pre_pt_root_id'\n",
    "        temp_df = dataframe[dataframe['pre_pt_root_id'] == unique_id]\n",
    "\n",
    "        # Extract the positions and corresponding post IDs\n",
    "        temp_positions = np.array(list(temp_df['pre_pt_position']))\n",
    "        temp_post_ids = temp_df['post_pt_root_id'].values\n",
    "\n",
    "        # Initialize a bouton counter and lists for bouton positions and bouton partners\n",
    "        bouton_counter = 0\n",
    "        bouton_positions = []\n",
    "        bouton_post_partners = []\n",
    "        visited = np.zeros(len(temp_positions), dtype=bool)\n",
    "\n",
    "        for i in range(len(temp_positions)):\n",
    "            if visited[i]:\n",
    "                continue\n",
    "\n",
    "            # Calculate distances from the current position to all others\n",
    "            distances = np.sqrt(np.sum((temp_positions - temp_positions[i]) ** 2, axis=1))\n",
    "\n",
    "            # Find indices of rows within the voxel threshold\n",
    "            close_points = np.where((distances <= voxel_threshold) & (~visited))[0]\n",
    "\n",
    "            # If 4 or more rows are within the voxel threshold, count it as a bouton\n",
    "            if len(close_points) >= synapse_threshold:\n",
    "                bouton_counter += 1\n",
    "                visited[close_points] = True\n",
    "\n",
    "                # Save the positions and unique post IDs of this bouton group\n",
    "                bouton_positions.append(temp_positions[close_points].tolist())\n",
    "                bouton_post_partners.append(list(set(temp_post_ids[close_points])))\n",
    "\n",
    "        # Combine groups that are within 1000 of each other\n",
    "        combined_positions = []\n",
    "        combined_post_partners = []\n",
    "\n",
    "        while bouton_positions:\n",
    "            group = bouton_positions.pop(0)\n",
    "            group_post_ids = bouton_post_partners.pop(0)\n",
    "            group = np.array(group)\n",
    "            to_merge = []\n",
    "\n",
    "            for idx, other_group in enumerate(bouton_positions):\n",
    "                other_group = np.array(other_group)\n",
    "                other_post_ids = bouton_post_partners[idx]\n",
    "                # Check if the current group is within 1000 of another group\n",
    "                distances = np.sqrt(np.sum((group[:, None] - other_group[None, :]) ** 2, axis=2))\n",
    "                if np.any(distances <= 300):\n",
    "                    to_merge.append((other_group, other_post_ids))\n",
    "\n",
    "            # Merge all nearby groups into the current group\n",
    "            for merge_group, merge_post_ids in to_merge:\n",
    "                group = np.vstack((group, merge_group))\n",
    "                group_post_ids = list(set(group_post_ids + merge_post_ids))\n",
    "                bouton_positions.remove(merge_group.tolist())\n",
    "                bouton_post_partners.remove(merge_post_ids)\n",
    "\n",
    "            combined_positions.append(group.tolist())\n",
    "            combined_post_partners.append(group_post_ids)\n",
    "\n",
    "        # Update bouton count after merging\n",
    "        bouton_counter = len(combined_positions)\n",
    "\n",
    "        # Initialize the results list for this `pre_pt_root_id`\n",
    "        results_by_bouton[unique_id] = []\n",
    "\n",
    "        # Create separate lists for each unique bouton's results\n",
    "        for i in range(bouton_counter):\n",
    "            synapse_variance, synapse_mean_distance = compute_xyz_metrics(combined_positions[i])\n",
    "            volume_ = find_volume(combined_positions[i])\n",
    "\n",
    "            results_by_bouton[unique_id].append({\n",
    "                \"pre_pt_root_id\": unique_id,\n",
    "                \"bouton_id\": i + 1,\n",
    "                \"bouton_positions\": combined_positions[i],\n",
    "                \"bouton_partners\": combined_post_partners[i],\n",
    "                \"total_boutons\": bouton_counter,  # Added total bouton count\n",
    "                \"bouton_volume\": volume_,   # Added bouton volume\n",
    "                \"synapse_variance\": synapse_variance, # Added synapse variance\n",
    "                \"synapse_mean_distance\": synapse_mean_distance\n",
    "            })\n",
    "\n",
    "    # Flatten results into a single list of dictionaries for DataFrame creation\n",
    "    flattened_results = [item for bouton_list in results_by_bouton.values() for item in bouton_list]\n",
    "    boutons_df = pd.DataFrame(flattened_results)\n",
    "\n",
    "    return boutons_df\n",
    "\n",
    "\n",
    "def find_volume(bouton_positions):\n",
    "    # Ensure bouton_positions is a NumPy array\n",
    "    bouton_positions = np.array(bouton_positions)\n",
    "    \n",
    "    # Check if bouton_positions has enough points for a ConvexHull\n",
    "    if bouton_positions.shape[0] < 4:\n",
    "        #print(\"Not enough points for ConvexHull. Returning volume=0.\")\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        hull = ConvexHull(bouton_positions)\n",
    "        volume = hull.volume\n",
    "    except Exception as e:\n",
    "        #print(f\"ConvexHull failed with error: {e}. Returning volume=0.\")\n",
    "        volume = 0\n",
    "    \n",
    "    return volume\n",
    "\n",
    "\n",
    "def split_lists_by_bouton_id(df):\n",
    "    \"\"\"\n",
    "    Splits the values in the specified columns of the DataFrame based on occurrences of \"1\" in the \"bouton_id\" column,\n",
    "    including preceding non-1 values and creating a new sublist for each \"1\".\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing columns with values to be split.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The transformed DataFrame with nested lists for specified columns.\n",
    "    \"\"\"\n",
    "    # Remove all none type columns\n",
    "    df = df.dropna(subset=['bouton_id'])\n",
    "    \n",
    "    # Columns to transform\n",
    "    columns_to_split = [\"bouton_id\", \"bouton_positions\", \"bouton_partners\", \"bouton_volume\", \"synapse_variance\", \\\n",
    "                        \"synapse_mean_distance\", \"pre_pt_root_id\"]\n",
    "\n",
    "    def split_row(row, columns_to_split):\n",
    "        # Lists to store the split values\n",
    "        new_column_values = {col: [] for col in columns_to_split}\n",
    "        \n",
    "        # Temporary storage for the current sublist\n",
    "        temp_values = {col: [] for col in columns_to_split}\n",
    "\n",
    "        for i, val in enumerate(row[\"bouton_id\"]):\n",
    "            if val == 1:\n",
    "                # Add the current sublist before the 1 (if any)\n",
    "                if temp_values[\"bouton_id\"]:  # Only add if temp list is not empty\n",
    "                    for col in columns_to_split:\n",
    "                        new_column_values[col].append(temp_values[col])\n",
    "                \n",
    "                # Start a new sublist for the 1 and include it\n",
    "                for col in columns_to_split:\n",
    "                    temp_values[col] = [row[col][i]]\n",
    "\n",
    "            else:\n",
    "                # Add the current value to the ongoing sublist\n",
    "                for col in columns_to_split:\n",
    "                    temp_values[col].append(row[col][i])\n",
    "\n",
    "        # Add any remaining values in temp_values to the final lists\n",
    "        if temp_values[\"bouton_id\"]:\n",
    "            for col in columns_to_split:\n",
    "                new_column_values[col].append(temp_values[col])\n",
    "\n",
    "        # Update the row for the specified columns\n",
    "        for col in columns_to_split:\n",
    "            row[col] = new_column_values[col]\n",
    "\n",
    "        return row\n",
    "\n",
    "    # Apply the splitting function to each row in the DataFrame\n",
    "    df = df.apply(lambda row: split_row(row, columns_to_split), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_bouton_data(df, columns_to_filter, threshold_upper=100000000, threshold_lower=900):\n",
    "    df = df.copy()\n",
    "    for index in df.index:\n",
    "        bouton_volumes = df.at[index, 'bouton_volume']\n",
    "        flat_bouton_volumes = [\n",
    "            item for sublist in bouton_volumes \n",
    "            for item in (sublist if isinstance(sublist, list) else [sublist])\n",
    "        ]\n",
    "        flat_bouton_volumes = [\n",
    "            v for v in flat_bouton_volumes if isinstance(v, (int, float)) and not isinstance(v, bool)\n",
    "        ]\n",
    "        if not flat_bouton_volumes or all(v == 0 for v in flat_bouton_volumes):\n",
    "            continue\n",
    "        if any(v > threshold_upper for v in flat_bouton_volumes) or all(v < threshold_lower for v in flat_bouton_volumes):\n",
    "            for col in columns_to_filter:\n",
    "                if col in df.columns:\n",
    "                    column_values = df.at[index, col]\n",
    "                    #if len(flat_bouton_volumes) != len(column_values):\n",
    "                    #    print(f\"Skipping row {index} due to length mismatch: bouton_volume={len(flat_bouton_volumes)}, {col}={len(column_values)}\")\n",
    "                    #   continue\n",
    "                    df.at[index, col] = [\n",
    "                        val for idx, val in enumerate(column_values) \n",
    "                        if threshold_lower <= flat_bouton_volumes[idx] <= threshold_upper\n",
    "                    ]\n",
    "        pre_pt_root_id_list = df.at[index, 'pre_pt_root_id']\n",
    "        # Flatten the list\n",
    "        pre_pt_root_id_flat = list(chain.from_iterable(pre_pt_root_id_list))\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        pre_pt_root_id_unique = [x for x in pre_pt_root_id_flat if not (x in seen or seen.add(x))]\n",
    "        df.at[index, 'Potential_Mossy_Partners'] = list(pre_pt_root_id_unique)                 \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_non_mossy_fibers_advanced(df):\n",
    "    \"\"\"\n",
    "    Advanced filtering for non-mossy fibers using trait-specific thresholds and combined conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe containing mossy fiber data.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered dataframe with non-mossy fibers removed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define trait-specific thresholds (customize these based on domain knowledge)\n",
    "    thresholds = {               \n",
    "        'bouton_volume': (50000, 20000000),  # Example range\n",
    "        'skeleton_length': (1000, 1000000),\n",
    "        'skeleton_branch_num': (0, 10),\n",
    "        'synapse_variance': (0, 10000),\n",
    "        'synapse_mean_distance': (50, 200)\n",
    "    }\n",
    "    \n",
    "    # Parse stringified lists\n",
    "    trait_columns = list(thresholds.keys())\n",
    "    for col in trait_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Apply filtering logic\n",
    "    for col, (lower, upper) in thresholds.items():\n",
    "        if col in df.columns:\n",
    "            # Flatten nested lists\n",
    "            df[f\"flat_{col}\"] = df[col].apply(\n",
    "                lambda x: [item for sublist in x for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "                if isinstance(x, list) else [x]\n",
    "            )\n",
    "            \n",
    "            # Remove entries with values outside the defined range\n",
    "            def filter_row(trait_values, mossy_fibers):\n",
    "                return [\n",
    "                    mossy_fiber\n",
    "                    for idx, mossy_fiber in enumerate(mossy_fibers)\n",
    "                    if idx < len(trait_values) and all(lower <= val <= upper for val in (trait_values[idx] if isinstance(trait_values[idx], list) else [trait_values[idx]]))\n",
    "                ]\n",
    "\n",
    "            df[f\"filtered_{col}\"] = df.apply(\n",
    "                lambda row: filter_row(row[f\"flat_{col}\"], row[\"MOSSY_FIBER\"])\n",
    "                if len(row[\"MOSSY_FIBER\"]) == len(row[f\"flat_{col}\"])\n",
    "                else row[\"MOSSY_FIBER\"], axis=1\n",
    "            )\n",
    "\n",
    "            # Update `MOSSY_FIBER` column\n",
    "            df[\"MOSSY_FIBER\"] = df[f\"filtered_{col}\"]\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    \n",
    "    intermediate_cols = [f\"flat_{col}\" for col in trait_columns] + [f\"filtered_{col}\" for col in trait_columns]\n",
    "    df = df.drop(columns=[col for col in intermediate_cols if col in df.columns], errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def reorder_dataframe_columns(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame to match a specified order.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with reordered columns.\n",
    "    \"\"\"\n",
    "    # Desired column order\n",
    "    new_order = [\n",
    "        \"bouton_partner\", \"Potential_Mossy_Partners\", \"pre_pt_root_id\", \n",
    "        \"bouton_id\", \"bouton_positions\", \"bouton_partners\", \"bouton_volume\",\n",
    "        \"synapse_variance\", \"synapse_mean_distance\", \n",
    "        \"presyn_segids\", \"num_presyn_segids\"\n",
    "    ]\n",
    "    \n",
    "    # Reorder columns based on the new order\n",
    "    return df[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3766ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(index, row, progress_bar):\n",
    "    \"\"\"Process a single row to calculate skeleton properties.\"\"\"\n",
    "    results = []  # To store results for all skeletons in this row\n",
    "    filtered_sids = []  # To store valid sids with branch numbers <= 10\n",
    "\n",
    "    for sid in row['Potential_Mossy_Partners']:\n",
    "        if sid > 0:\n",
    "            retries = 0\n",
    "            max_retries = 5  # Maximum number of retries\n",
    "            wait_time = 120  # Wait time in seconds between retries\n",
    "\n",
    "            while retries < max_retries:\n",
    "                try:\n",
    "                    # Fetch the skeleton\n",
    "                    skel = pcg_skel.pcg_skeleton(root_id=sid, client=client, root_point_resolution=[1, 1, 1])\n",
    "                    vertices = skel.vertices  # Skeleton vertices\n",
    "                    edges = skel.edges  # Skeleton edges\n",
    "\n",
    "                    # Calculate total skeleton length\n",
    "                    edge_lengths = np.linalg.norm(vertices[edges[:, 0]] - vertices[edges[:, 1]], axis=1)\n",
    "                    total_length = edge_lengths.sum()\n",
    "\n",
    "                    # Build a graph from edges to represent the skeleton\n",
    "                    graph = nx.Graph()\n",
    "                    for i, edge in enumerate(edges):\n",
    "                        graph.add_edge(edge[0], edge[1], length=edge_lengths[i])\n",
    "\n",
    "                    # Identify branch points (nodes with degree > 2)\n",
    "                    branch_points = [node for node in graph.nodes if graph.degree[node] > 2]\n",
    "\n",
    "                    # Traverse all branches to measure their lengths\n",
    "                    branch_lengths = []\n",
    "                    visited_edges = set()\n",
    "\n",
    "                    for branch_point in branch_points:\n",
    "                        for neighbor in graph.neighbors(branch_point):\n",
    "                            edge = tuple(sorted((branch_point, neighbor)))\n",
    "                            if edge not in visited_edges:\n",
    "                                visited_edges.add(edge)\n",
    "                                # Perform DFS to measure the branch length\n",
    "                                branch_length = 0\n",
    "                                current_node = neighbor\n",
    "                                previous_node = branch_point\n",
    "                                while True:\n",
    "                                    branch_length += graph[previous_node][current_node]['length']\n",
    "                                    visited_edges.add(tuple(sorted((previous_node, current_node))))\n",
    "                                    neighbors = list(graph.neighbors(current_node))\n",
    "                                    neighbors.remove(previous_node)  # Remove the previous node from consideration\n",
    "\n",
    "                                    if len(neighbors) == 1:  # Continue to the next node\n",
    "                                        previous_node = current_node\n",
    "                                        current_node = neighbors[0]\n",
    "                                    else:  # Reached a branch point or end of branch\n",
    "                                        break\n",
    "                                branch_lengths.append(branch_length)\n",
    "\n",
    "                    # Only keep skeletons with branch numbers <= 10 and valid branch lengths\n",
    "                    num_branches = len(branch_lengths)\n",
    "                    if num_branches <= 15:\n",
    "                        # Check if more than 6 branch lengths exceed 100,000\n",
    "                        large_branch_count = sum(1 for length in branch_lengths if length > 1000)\n",
    "                        if large_branch_count <= 6:\n",
    "                            results.append((total_length, num_branches, branch_lengths))\n",
    "                            filtered_sids.append(sid)\n",
    "                    break  # Successfully processed this skeleton, exit retry loop\n",
    "\n",
    "                except Exception as e:\n",
    "                    retries += 1\n",
    "                    if retries < max_retries:\n",
    "                        print(f\"Error processing skeleton for ID {sid}: {e}. Retrying in {wait_time} seconds...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"Error processing skeleton for ID {sid} after {max_retries} retries: {e}. Skipping...\")\n",
    "\n",
    "        else:\n",
    "            # Handle invalid skeleton IDs\n",
    "            print(f\"Invalid skeleton ID {sid}. Skipping...\")\n",
    "\n",
    "        # Update progress bar for each skeleton ID\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Return all results for this row and the filtered sids\n",
    "    return index, results, filtered_sids\n",
    "\n",
    "\n",
    "# Main function\n",
    "def mossy_fiber_skeleton_sorter(df):\n",
    "    df['skeleton_length'] = None  # List of total lengths for each SID\n",
    "    df['skeleton_branch_num'] = None  # List of branch numbers for each SID\n",
    "    df['skeleton_branch_length'] = None  # List of lists of branch lengths for each SID\n",
    "    df['mossy_fibers'] = None  # Filtered list of valid sids\n",
    "\n",
    "    results = []\n",
    "    total_tasks = sum(len(row['Potential_Mossy_Partners']) for _, row in df.iterrows())\n",
    "    progress_bar = tqdm(total=total_tasks, desc=\"Processing skeletons sequentially\", position=0)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        results.append(process_row(index, row, progress_bar))\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Update the DataFrame with results\n",
    "    for index, result_list, filtered_sids in results:\n",
    "        skeleton_length = [result[0] for result in result_list]  # List of total lengths\n",
    "        skeleton_branch_num = [result[1] for result in result_list]  # List of branch numbers\n",
    "        skeleton_branch_length = [result[2] for result in result_list]  # List of lists of branch lengths\n",
    "\n",
    "        # Update the DataFrame\n",
    "        df.at[index, 'skeleton_length'] = skeleton_length\n",
    "        df.at[index, 'skeleton_branch_num'] = skeleton_branch_num\n",
    "        df.at[index, 'skeleton_branch_length'] = skeleton_branch_length\n",
    "        df.at[index, 'mossy_fibers'] = filtered_sids\n",
    "    \n",
    "    # Remove the Potential_Mossy_Partners column\n",
    "    df = df.drop(columns=['Potential_Mossy_Partners'])\n",
    "    df.rename(columns={'bouton_partner': 'POSTSYNAPTIC_CELL'}, inplace=True)\n",
    "    df.rename(columns={'mossy_fibers': 'MOSSY_FIBER'}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_row_value_counts(df, column='MOSSY_FIBER', new_column_name='total_mossy_fiber_num'):\n",
    "    \"\"\"\n",
    "    Adds a new column to the DataFrame with the count of values in the list within a specified column for each row.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): The column containing lists (or strings of lists) to count values from.\n",
    "        new_column_name (str): Name of the new column to add with the counts.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the new column added.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"The specified column '{column}' is not in the DataFrame.\")\n",
    "    \n",
    "    df[new_column_name] = df[column].apply(len)\n",
    "    \n",
    "    # Sort the DataFrame by 'total_mossy_fiber_num' in descending order\n",
    "    df_sorted = df.sort_values(by=new_column_name, ascending=False)\n",
    "\n",
    "    # Reset the index if needed (optional)\n",
    "    df_sorted = df_sorted.reset_index(drop=True)\n",
    "    \n",
    "    # Filter rows where 'total_mossy_fiber_num' is not 0\n",
    "    df_filtered = df_sorted.loc[df_sorted['total_mossy_fiber_num'] != 0]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def reorder_dataframe_columns_2(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame to match a specified order.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with reordered columns.\n",
    "    \"\"\"\n",
    "    # Desired column order\n",
    "    new_order = [\n",
    "        \"POSTSYNAPTIC_CELL\", \"MOSSY_FIBER\", \"total_mossy_fiber_num\", \n",
    "        \"bouton_id\", \"bouton_positions\", \"bouton_partners\", \"bouton_volume\",\n",
    "        \"skeleton_length\", \"skeleton_branch_num\", \"skeleton_branch_length\", \n",
    "        \"synapse_variance\", \"synapse_mean_distance\", \n",
    "        \"presyn_segids\", \"num_presyn_segids\", \"pre_pt_root_id\"\n",
    "    ]\n",
    "    \n",
    "    # Reorder columns based on the new order\n",
    "    return df[new_order]\n",
    "\n",
    "\n",
    "def scale_xyz_column(dataframe, column_name, new_col_name):\n",
    "    \"\"\"\n",
    "    Scales nested lists of coordinates in the specified column of a DataFrame.\n",
    "    Each X and Y value is divided by 18, and each Z value is divided by 45.\n",
    "    The nested structure is preserved, and the scaled values are saved in a new column.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The input DataFrame.\n",
    "    column_name (str): The name of the column containing the nested XYZ lists.\n",
    "    new_col_name (str): The name of the new column to store the scaled coordinates.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with the scaled coordinates added as a new column.\n",
    "    \"\"\"\n",
    "    def scale_coordinates(item):\n",
    "        if isinstance(item, list) and all(isinstance(i, (int, float)) for i in item) and len(item) == 3:\n",
    "            # If it's a list of 3 numbers, scale them\n",
    "            x, y, z = item\n",
    "            return [x / 18, y / 18, z / 45]\n",
    "        elif isinstance(item, list):\n",
    "            # If it's a list, process each element recursively\n",
    "            return [scale_coordinates(sub_item) for sub_item in item]\n",
    "        else:\n",
    "            # If it's neither a list nor an XYZ coordinate, return it as is\n",
    "            return item\n",
    "\n",
    "    # Apply the recursive scaling function to the specified column\n",
    "    dataframe[new_col_name] = dataframe[column_name].apply(scale_coordinates)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def scale_bouton_volume(df, bouton_volume_col, new_bouton_col_name, scale_factor):\n",
    "    \"\"\"\n",
    "    Scales bouton volume values in a specific column of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to process.\n",
    "    - bouton_volume_col (str): Name of the column containing bouton volumes to scale.\n",
    "    - new_bouton_col_name (str): Name of the new column to store scaled bouton volumes.\n",
    "    - scale_factor (float): Factor by which to scale the bouton volumes.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with the scaled bouton volume column.\n",
    "    \"\"\"\n",
    "    def scale_volume(nested_list):\n",
    "        return [[val * scale_factor for val in sublist] for sublist in nested_list]\n",
    "    \n",
    "    df[new_bouton_col_name] = df[bouton_volume_col].apply(scale_volume)\n",
    "    return df\n",
    "\n",
    "\n",
    "def reorder_dataframe_columns_3(df):\n",
    "    \"\"\"\n",
    "    Reorders the columns of the DataFrame to match a specified order.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with reordered columns.\n",
    "    \"\"\"\n",
    "    # Desired column order\n",
    "    new_order = [\n",
    "        \"POSTSYNAPTIC_CELL\", \"MOSSY_FIBER\", \"total_mossy_fiber_num\", \n",
    "        \"bouton_id\", \"bouton_positions\", \"bouton_positions_um\", \"bouton_partners\", \n",
    "        \"bouton_volume\", \"bouton_volume_um\", \"skeleton_length\", \"skeleton_branch_num\", \n",
    "        \"skeleton_branch_length\", \"synapse_variance\", \"synapse_mean_distance\", \n",
    "        \"presyn_segids\", \"num_presyn_segids\", \"pre_pt_root_id\"\n",
    "    ]\n",
    "    \n",
    "    # Reorder columns based on the new order\n",
    "    return df[new_order]\n",
    "\n",
    "\n",
    "def filter_leftover_bouton_data(df):\n",
    "    # Define the columns that need to have corresponding lists removed\n",
    "    columns_to_filter = [\n",
    "        \"bouton_id\", \"bouton_partners\", \"synapse_mean_distance\",\n",
    "        \"synapse_variance\", \"bouton_volume\", \"bouton_volume_um\",\n",
    "        \"bouton_positions_um\", \"bouton_positions\"\n",
    "    ]\n",
    "    \n",
    "    # Iterate through each row of the DataFrame with a progress bar\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Filtering DataFrame\"):\n",
    "        # Extract the lists of lists for the current row\n",
    "        pre_pt_root_id = row[\"pre_pt_root_id\"]\n",
    "        mossy_fiber = row[\"MOSSY_FIBER\"]\n",
    "        \n",
    "        # Convert to Python objects if they are strings\n",
    "        if isinstance(pre_pt_root_id, str):\n",
    "            pre_pt_root_id = eval(pre_pt_root_id)\n",
    "        if isinstance(mossy_fiber, str):\n",
    "            mossy_fiber = eval(mossy_fiber)\n",
    "        \n",
    "        # Find the indices of the lists in pre_pt_root_id that match mossy_fiber\n",
    "        valid_indices = [i for i, lst in enumerate(pre_pt_root_id) if lst[0] in mossy_fiber]\n",
    "        \n",
    "        # Filter the lists in pre_pt_root_id and the other columns\n",
    "        df.at[index, \"pre_pt_root_id\"] = [pre_pt_root_id[i] for i in valid_indices]\n",
    "        for col in columns_to_filter:\n",
    "            col_data = row[col]\n",
    "            if isinstance(col_data, str):\n",
    "                col_data = eval(col_data)\n",
    "            df.at[index, col] = [col_data[i] for i in valid_indices]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c124556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_known_pyr_cell(df):\n",
    "\n",
    "    df_pyr = pd.read_csv('all_pyramidal_cells - Copy of MF-pyr.csv')\n",
    "\n",
    "    updated_segid_list = client.chunkedgraph.get_roots(df_pyr['supervoxel'])\n",
    "    \n",
    "    \n",
    "    print(len(df[\"bouton_partner\"]))\n",
    "\n",
    "    df = df[df[\"bouton_partner\"].apply(lambda x: isinstance(x, (list, tuple, set)) and all(int(i) in updated_segid_list for i in x))]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7abb627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Read Starting CSV, DF Length: 10\n",
      "----------------------------------------------\n",
      "----------------------------------------------\n",
      "Segids Updated, DF Length: 10\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Mossy Fiber Synapses: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing boutons for chunk [648518346439917587, 648518346429693973, 648518346440572966, 648518346442407984, 648518346446209074, 648518346454335564, 648518346439524444, 648518346436247690, 648518346451583119, 648518346438869147]: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.\n",
      "----------------------------------------------\n",
      "Boutons Processed, DF Length: 1\n",
      "----------------------------------------------\n",
      "----------------------------------------------\n",
      "Bouton Partners Found, DF Length: 9\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Mossy Fiber Synapses: 100%|██████████| 9/9 [00:02<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Partner Cell Analysis, DF Length: 2\n",
      "----------------------------------------------\n",
      "----------------------------------------------\n",
      "Splitting boutons in dataframe, DF Length: 2\n",
      "----------------------------------------------\n",
      "----------------------------------------------\n",
      "Bouton Volumes Processed, DF Length: 2\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing skeletons sequentially: 100%|██████| 95/95 [01:58<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Partner Skeleton Analysis, DF Length: 2\n",
      "----------------------------------------------\n",
      "----------------------------------------------\n",
      "Redorder and Count total MF, DF Length: 2\n",
      "----------------------------------------------\n",
      "----------------------------------------------\n",
      "Filtered by Z score, DF Length: 1\n",
      "----------------------------------------------\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering DataFrame: 100%|████████████████████| 1/1 [00:00<00:00, 1499.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Fix Left Over Bouton Data, DF Length: 1\n",
      "----------------------------------------------\n",
      "PROCESS FINISHED\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The basics of how this works.\n",
    "\n",
    "This code takes in the csv file of already found mossy fibers. Then it updates all of their segids \n",
    "to what they currently are. Then it looks for boutons in all of these \"mossy fibers\" by finding how \n",
    "many synapses (minimum 8) are within the thresholded distance (500 voxels). Each time this is found \n",
    "this is considered a bouton and a number is added to the bouton_id as well as other information like \n",
    "the volume etc. So if a row has a bouton_id of [1,2,3,4] there are four boutons. And each bouton \n",
    "corresponds to the pre_pt_root_id in the list (would look something like this: [648518346449706895, \n",
    "648518346449706895, 648518346449706895, 648518346449706895]). We then look for all of the synaptic \n",
    "partners of these mossy fibers (using bouton_parters) and then find all cells postsynaptic to them. \n",
    "These are assumed to be CA3 cells, either inhibitory, glial, or pyramidal. Now we look at these CA3\n",
    "cells' presynaptic partner and do the same bouton_detection to try to find all mossy fibers connecting\n",
    "to these CA3 cells. Finally we do some sorting so the lists in each row make sense and correspond with \n",
    "the pre_pt_root_id, etc. and use the skeleton of each mossy fiber to help weed out any non mossy fibers. \n",
    "Still needs more work to weed out axons, but pretty good overall. Takes about 30 hours. \n",
    "\n",
    "\"\"\"\n",
    "current_date = datetime.now()\n",
    "formatted_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "df = pd.read_csv(\"CA3 proofreading 2 - 16.MF_identification_EH_1stPriority_ONLY.csv\")\n",
    "df = df.iloc[0:10]\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Read Starting CSV, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "#df = update_segids_df(df, super_voxel_col=\"pre_pt_root_id\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Segids Updated, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "current_date = datetime.now().strftime('%Y%m%d') \n",
    "new_name = f\"updated_segids_{current_date}\"\n",
    "df.rename(columns={\"Potential mossy fibers\": new_name}, inplace=True)\n",
    "\n",
    "# Takes about 2.5 hours. \n",
    "df = bouton_processor(df, limit_TF=False, limit=5)\n",
    "df.to_csv(f'V2_Step_1_BoutonProcessor_{formatted_date}.csv', index=False)\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Boutons Processed, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "df = find_bouton_partners(df)\n",
    "df.to_csv(f'V2_Step_2_BoutonPartners_{formatted_date}.csv', index=False)\n",
    "\n",
    "\n",
    "#print(\"----------------------------------------------\")\n",
    "#print(\"NEW NEW NEW - - - - - ONLY KNOWN PYR CELLS\")\n",
    "#print(\"----------------------------------------------\")\n",
    "#df = only_known_pyr_cell(df)\n",
    "#print(\"----------------------------------------------\")\n",
    "#print(f\"Pyr Only Partners Left, DF Length: {len(df)}\")\n",
    "#print(\"----------------------------------------------\")\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Bouton Partners Found, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "df = nonMossy_partner_analysis(df)\n",
    "# Seems to take about 6-8 hours \n",
    "df.to_csv(f'V2_Step_3_NonMossyPartners_{formatted_date}.csv', index=False)\n",
    "df_1 = df.head(1)  \n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Partner Cell Analysis, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "df = split_lists_by_bouton_id(df)\n",
    "df_2 = df.head(1)\n",
    "df.to_csv(f'V2_Step_4_CellsSplit_{formatted_date}.csv', index=False)\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Splitting boutons in dataframe, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "df = reorder_dataframe_columns(df)\n",
    "df_3 = df.head(1)\n",
    "df = filter_bouton_data(df, columns_to_filter=['bouton_id', 'pre_pt_root_id', \\\n",
    "                        'bouton_positions', 'bouton_partners', 'synapse_variance', \\\n",
    "                        'synapse_mean_distance', 'bouton_volume'])\n",
    "df.to_csv(f'V2_Step_5_BoutonFilteredMossyFibers_{formatted_date}.csv', index=False)\n",
    "df_post = df\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Bouton Volumes Processed, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "df = mossy_fiber_skeleton_sorter(df)\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Partner Skeleton Analysis, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "df = add_row_value_counts(df, column='MOSSY_FIBER', new_column_name='total_mossy_fiber_num')\n",
    "df = reorder_dataframe_columns_2(df)\n",
    "df.to_csv(f'V2_Step_6_SkeletonFilteredMossyFibers_{formatted_date}.csv', index=False)\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Redorder and Count total MF, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "df = filter_non_mossy_fibers_advanced(df)\n",
    "df = add_row_value_counts(df, column='MOSSY_FIBER', new_column_name='total_mossy_fiber_num')\n",
    "df = scale_xyz_column(df, column_name='bouton_positions', new_col_name='bouton_positions_um')\n",
    "df = scale_bouton_volume(df, bouton_volume_col='bouton_volume', new_bouton_col_name='bouton_volume_um', scale_factor=14.58)\n",
    "df = reorder_dataframe_columns_3(df)\n",
    "df.to_csv(f'V2_Step_7_FULL_MOSSYFIBER_DATASET_{formatted_date}.csv', index=False)\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Filtered by Z score, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"----------------------------------------------\")\n",
    "df = filter_leftover_bouton_data(df)\n",
    "df.to_csv(f'V2_Step_8_FULL_MOSSYFIBER_DATASET_{formatted_date}.csv', index=False)\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"Fix Left Over Bouton Data, DF Length: {len(df)}\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"PROCESS FINISHED\")\n",
    "print(\"----------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
