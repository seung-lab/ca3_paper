{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "import plotly.subplots as sp\n",
    "import plotly.io as pio\n",
    "import os\n",
    "from cloudvolume import CloudVolume as cv\n",
    "from caveclient import CAVEclient\n",
    "import seaborn as sns \n",
    "import scipy.sparse\n",
    "import meshparty\n",
    "import pygsheets\n",
    "\n",
    "vol =cv('graphene://https://minnie.microns-daf.com/segmentation/table/zheng_ca3', agglomerate=True, use_https=True)\n",
    "client = CAVEclient('zheng_ca3')\n",
    "\n",
    "isotropy = [5,5,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f319bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = pygsheets.authorize(service_file = \"./hippca3-8126bea0d603.json\")\n",
    "sheet = gc.open('all_pyramidal_cells')\n",
    "worksheet = sheet.worksheet('title','all_pyr_v2_remove_dupes')\n",
    "pc_df = worksheet.get_as_df()\n",
    "str_columns = worksheet.get_as_df(numerize=False)['nul_segids']\n",
    "pc_df['nul_segids'] = str_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update all_pyr_v2_remove_dupes sheet from CA3 proofreading sheets 1.pyramidal_cell_list, 6.Additional_pyramidal\n",
    "\n",
    "sheet2 = gc.open('CA3 proofreading')\n",
    "worksheet2 = sheet2.worksheet('title','1.pyramidal_cell_list')\n",
    "pc_df2 = worksheet2.get_as_df(start='A9')\n",
    "str_columns = worksheet2.get_as_df(start='A9', numerize=False)['segid (Nuclei table segID)']\n",
    "pc_df2['segid (Nuclei table segID)'] = str_columns\n",
    "\n",
    "worksheet3 = sheet2.worksheet('title','6.Additional_pyramidal')\n",
    "pc_df3 = worksheet3.get_as_df(start='A9')\n",
    "str_columns = worksheet3.get_as_df(start='A9', numerize=False)['segid (Nuclei table segID)']\n",
    "pc_df3['segid (Nuclei table segID)'] = str_columns\n",
    "\n",
    "for i in range(0, len(pc_df)):\n",
    "    nuc_seg_id = pc_df['nul_segids'][i]\n",
    "    row_idx2 = np.where(nuc_seg_id == pc_df2['segid (Nuclei table segID)'].values)[0]\n",
    "    if row_idx2.size == 1: \n",
    "        pc_df.loc[i, 'Final Link'] = pc_df2['Final Link'].values[row_idx2][0] \n",
    "    else:\n",
    "        row_idx3 = np.where(nuc_seg_id == pc_df3['segid (Nuclei table segID)'].values)[0]\n",
    "        pc_df.loc[i, 'Final Link'] = pc_df3['Final Link'].values[row_idx3][0] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei_coord_in_vol = pc_df[['x','y','z']].to_numpy()\n",
    "latest_roots_PC = []\n",
    "print('Getting latest roots for PC...')  \n",
    "cell_ids_scatter = vol.scattered_points(nuclei_coord_in_vol)\n",
    "for i in range(0, len(nuclei_coord_in_vol)):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    cell_ID = [cell_ids_scatter.get(tuple(nuclei_coord_in_vol[i,:]))]\n",
    "    if cell_ID[0] == 0:\n",
    "        cell_ID = np.array([vol[nuclei_coord_in_vol[i,0]-150, nuclei_coord_in_vol[i,1], nuclei_coord_in_vol[i,2]].squeeze()])\n",
    "    if cell_ID[0] == 0:\n",
    "        cell_ID = np.array([vol[nuclei_coord_in_vol[i,0], nuclei_coord_in_vol[i,1]+150, nuclei_coord_in_vol[i,2]].squeeze()])\n",
    "    if cell_ID[0] == 0:\n",
    "        cell_ID = np.array([vol[nuclei_coord_in_vol[i,0], nuclei_coord_in_vol[i,1]-150, nuclei_coord_in_vol[i,2]].squeeze()])\n",
    "    if cell_ID[0] == 0:\n",
    "        print(\"Failed to find neuron segment ID using nuclei center\")\n",
    "    else:\n",
    "        if client.chunkedgraph.is_latest_roots(cell_ID[0]):\n",
    "            latest_roots_PC.append(cell_ID[0])\n",
    "        else:\n",
    "            latest_roots_PC.append(client.chunkedgraph.suggest_latest_roots(cell_ID[0]))\n",
    "latest_roots_PC_str = [[str(num)] for num in latest_roots_PC]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c489d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latest_roots_PC_str[0:10])\n",
    "\n",
    "final_links_matrix = []\n",
    "for i in range(0,len(pc_df)):\n",
    "    final_links_matrix.append([pc_df['Final Link'].values[i]])\n",
    "\n",
    "#worksheet.update_values('P2', latest_roots_PC_str)\n",
    "worksheet.update_values('AC2', final_links_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35469f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet.update_values('AC2', final_links_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 skeleton\n",
    "\n",
    "import pcg_skel\n",
    "import meshparty\n",
    "import time\n",
    "from nglui import parser\n",
    "\n",
    "pyc_i_with_ais = []\n",
    "ais_all = []\n",
    "nuc_coord_all = []\n",
    "ais_error= []\n",
    "\n",
    "for i in range(0, len(latest_roots_PC)):\n",
    "    if i not in [190,284,883]:\n",
    "        link = pc_df['Final Link'][i]\n",
    "        time.sleep(0.2)\n",
    "        if link != '':\n",
    "            this_state_id = link.split('/')[-1]\n",
    "            json_info = client.state.get_state_json(state_id =this_state_id)\n",
    "            layerNames = parser.layer_names(json_info)\n",
    "            ais_tab = [i for i, s in enumerate(layerNames) if \"AIS\" in s]\n",
    "            try:\n",
    "                pts, tags = parser.point_annotations(json_info, layerNames[ais_tab[0]], tags=True)\n",
    "            except:\n",
    "                ais_error.append(i+2)\n",
    "                continue        \n",
    "                \n",
    "            if pts != []:\n",
    "                pyc_i_with_ais.append(i)\n",
    "                ais = np.array([pts[0][0] /18 * 4, pts[0][1] /18 * 4, pts[0][2] /45 * 40])* np.array([0.018,0.018,0.045])\n",
    "                ais_all.append(ais)\n",
    "                nuc_coord = np.array(nuclei_coord_in_vol[i,:])* np.array([0.018,0.018,0.045])\n",
    "                nuc_coord_all.append(nuc_coord)\n",
    "                try:\n",
    "                    skel = pcg_skel.pcg_skeleton(root_id=latest_roots_PC[i], client=client, root_point=nuc_coord, collapse_soma=False, collapse_radius=50)\n",
    "                    meshparty.skeleton_io.export_to_swc(skel, './skel/pyc_with_axon/pyc_' + str(i) + '.swc', node_labels=None, radius=None, \n",
    "                                                    header=None, xyz_scaling=1000, resample_spacing=500, interp_kind='linear', \n",
    "                                                    tip_length_ratio=0.25, avoid_root=True)\n",
    "                    #meshparty.skeleton_io.export_to_swc(skel, './skel/pyc_with_axon/pyc_' + str(i) + '.swc', node_labels=None, radius=None, \n",
    "                    #                                header=None, xyz_scaling=1000, tip_length_ratio=0.25, avoid_root=True)\n",
    "                    print(i)\n",
    "                except Exception as e:\n",
    "                    print('skipping iteration ' + str(i))\n",
    "                    continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_skeleton(graph, split_node, nuclei_node):\n",
    "    graph.remove_node(split_node)\n",
    "    ccs = list(nx.connected_components(graph))\n",
    "    subgraphs = [graph.subgraph(cc).copy() for cc in ccs]\n",
    "    if len(subgraphs) >= 2:        \n",
    "        if nuclei_node in subgraphs[0]:        \n",
    "            return subgraphs[1], subgraphs[0]\n",
    "        elif nuclei_node in subgraphs[1]:\n",
    "            return subgraphs[0], subgraphs[1]\n",
    "        else:\n",
    "            print('cannot select the axon graph')            \n",
    "    else:\n",
    "        print('skeleton split failed.... i= ' + str(i))\n",
    "        return subgraphs[0], None\n",
    "\n",
    "def get_nearest_skel_node(skel, p):\n",
    "    skel['distance'] = np.sqrt(\n",
    "        (skel['x'] - p[0]) ** 2 +\n",
    "        (skel['y'] - p[1]) ** 2 +\n",
    "        (skel['z'] - p[2]) ** 2\n",
    "    )\n",
    "    nearest_node = skel.loc[skel['distance'].idxmin()]\n",
    "    return nearest_node\n",
    "\n",
    "def euc_distance(n1,n2):\n",
    "    return np.sqrt((n1['x'] - n2['x'])**2 + (n1['y']-n2['y'])**2 + (n1['z'] - n2['z'])**2)\n",
    "\n",
    "\n",
    "columns = ['id','type','x','y','z','radius','parent']\n",
    "#axon_lengths = []\n",
    "#axon_lengths.append(-1)\n",
    "for i in range(846, len(pyc_i_with_ais)):\n",
    "    if pyc_i_with_ais[i] in [1019,1060,1374,1403,1641,1619,1635,1718,1806,1820]:   # wrong AIS and minimal axon\n",
    "        axon_lengths.append(-1)\n",
    "    else:\n",
    "        j = pyc_i_with_ais[i]\n",
    "        sk = pd.read_csv('./skel/pyc_with_axon/pyc_' + str(j) + '.swc', delim_whitespace=True, names=columns)\n",
    "        G = nx.Graph()\n",
    "        for index, row in sk.iterrows():\n",
    "            G.add_node(row['id'], pos=(row['x'], row['y'], row['z']))\n",
    "        for index, row in sk.iterrows():\n",
    "            if row['parent'] != -1:\n",
    "                node1 = sk.loc[sk['id'] == row['id']].iloc[0]\n",
    "                node2 = sk.loc[sk['id'] == row['parent']].iloc[0]\n",
    "                dist = euc_distance(node1, node2)            \n",
    "                G.add_edge(row['id'], row['parent'], weight=dist)\n",
    "        ais_node_id = get_nearest_skel_node(sk, ais_all[i])\n",
    "        nuc_node_id = get_nearest_skel_node(sk, nuc_coord_all[i])\n",
    "        subgraph_axon, subgraph_dend = split_skeleton(G, ais_node_id['id'], nuc_node_id['id'])\n",
    "        axon_nodes = subgraph_axon.nodes\n",
    "        axon_df = sk[sk['id'].isin(axon_nodes)]\n",
    "        axon_len = subgraph_axon.size(weight='weight')\n",
    "        if subgraph_dend == None:\n",
    "            axon_lengths.append(-1)\n",
    "        else:\n",
    "            axon_lengths.append(axon_len)\n",
    "        scatter1=go.Scatter3d(x=sk['x'],y=sk['y'],z=sk['z'],mode='markers', marker=dict(size=2))\n",
    "        scatter2=go.Scatter3d(x=axon_df['x'],y=axon_df['y'],z=axon_df['z'],mode='markers', marker=dict(size=2))\n",
    "        scatter_ais = go.Scatter3d(x=[ais_all[i][0]], y=[ais_all[i][1]], z=[ais_all[i][2]], mode='markers', marker=dict(size=5, color='tomato'))\n",
    "        layout = go.Layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z',aspectmode=\"data\"))\n",
    "        fig = go.Figure(data=[scatter1,scatter2, scatter_ais], layout=layout)\n",
    "        pio.write_image(fig, './skel/pyc_with_axon/fig/pyc_'+str(pyc_i_with_ais[i])+'_'+str(int(axon_len))+'.jpeg', format='jpeg', scale=2)\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e48410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AIS coordinates and write to spreadsheet \n",
    "\n",
    "\n",
    "import time\n",
    "from nglui import parser\n",
    "\n",
    "pyc_i_with_ais = []\n",
    "ais_all = []\n",
    "ais_error= []\n",
    "\n",
    "#for i in range(0,1):\n",
    "for i in range(0, len(latest_roots_PC)):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    #if i not in [190,284,883,1438]:\n",
    "    if i not in [793]:\n",
    "        link = pc_df['Final Link'][i]\n",
    "        time.sleep(0.1)\n",
    "        if link != '':\n",
    "            this_state_id = link.split('/')[-1]\n",
    "            json_info = client.state.get_state_json(state_id =this_state_id)\n",
    "            layerNames = parser.layer_names(json_info)\n",
    "            ais_tab = [i for i, s in enumerate(layerNames) if \"AIS\" in s]\n",
    "            try:\n",
    "                pts, tags = parser.point_annotations(json_info, layerNames[ais_tab[0]], tags=True)\n",
    "            except:\n",
    "                ais_error.append(i+2)\n",
    "                ais_all.append(['NA','NA','NA'])\n",
    "                continue        \n",
    "                \n",
    "            if pts != []:\n",
    "                pyc_i_with_ais.append(i)\n",
    "                ais = [round(pts[0][0] /18 * 4), round(pts[0][1] /18 * 4), round(pts[0][2] /45 * 40)]\n",
    "                ais_all.append(ais)\n",
    "            else:\n",
    "                ais_all.append(['NA','NA','NA'])\n",
    "        else:\n",
    "            ais_all.append(['NA','NA','NA'])\n",
    "    else:\n",
    "        ais_all.append(['NA','NA','NA'])\n",
    "            \n",
    "\n",
    "#worksheet.update_values('AI2', ais_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet.update_values('AI2', ais_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db68f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axon length failures\n",
    "\n",
    "pyc_i_failed = [29,219,245,370,557,561,603,646,747,792,796,823,830,910,958,962,1374,1404,1408,\n",
    "                1410,1418,1420,1434,1479,1487,1505,1523,1526,1542,1554,1566,1572,1575,1595,1617,1638,\n",
    "                1641,1647,1651,1661,1663,1673,1684,1718,1728,1753,1763,1802,1807,1811,1837]\n",
    "\n",
    "pyc_i_manual = [29,561,747,792,823,830,\n",
    "                910,958,1374,1404,1408,1415,1418,1420,1434,1479,1487,1505,\n",
    "                1554,1566,1572,1575,1595,1617,1638,1641,1661,1673,\n",
    "                1684,1728,1753,1763,1765,1794,1807,1811,1837]\n",
    "\n",
    "\n",
    "ais_manual = [[44879,52318,832],[42444,46893,1307],[54664,52458,1541],[57534,50874,893],[46066,48031,1331],[61540,58088,939],\n",
    "             [60316,65507,1386],[43150,45624,851],[28464,71694,675],[41238, 44676, 1065],[31790, 63987, 1060],[62720, 64129, 1360],[48812,47481,1166],[39731, 43786, 831],[53616, 49740, 1083],[34234,51794,599],[53567, 47324, 924],[61290,71906,846],\n",
    "            [54480,45982,743],[43197, 45603, 916],[31765, 60128, 831],[56072,52033,760],[53682, 48715, 1192],[30323, 64754, 821],[61157, 62726, 814],[50536, 41175, 627],[39767, 44799, 1022],[40726, 47235, 773],\n",
    "                [30285, 72805, 1090],[57200, 52433, 578],[53401, 48812, 901],[33538, 51888, 474],[33619, 51964,423],[52453, 46064, 550],[42942, 45535, 904],[56855, 52544,511],[49155, 38795, 670]\n",
    "             ]\n",
    "\n",
    "pyc_i_axon_self_touch = [245,370,557,796,962,1410,1523,1526,1651,1663,1753,1794]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(pyc_i_manual) - set(pyc_i_with_ais))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "axon_lengths_copy = copy.deepcopy(axon_lengths)\n",
    "axon_lengths_additional = []\n",
    "\n",
    "for i in range(0, len(pyc_i_manual)):\n",
    "    j = pyc_i_manual[i]\n",
    "    idxx = pyc_i_with_ais.index(j)\n",
    "    sk = pd.read_csv('./skel/pyc_with_axon/pyc_' + str(j) + '.swc', delim_whitespace=True, names=columns)\n",
    "    G = nx.Graph()\n",
    "    for index, row in sk.iterrows():\n",
    "        G.add_node(row['id'], pos=(row['x'], row['y'], row['z']))\n",
    "\n",
    "    for index, row in sk.iterrows():\n",
    "        if row['parent'] != -1:\n",
    "            node1 = sk.loc[sk['id'] == row['id']].iloc[0]\n",
    "            node2 = sk.loc[sk['id'] == row['parent']].iloc[0]\n",
    "            dist = euc_distance(node1, node2)            \n",
    "            G.add_edge(row['id'], row['parent'], weight=dist)\n",
    "\n",
    "    ais_coord = np.array(ais_manual[i]) * np.array([0.018,0.018,0.045])\n",
    "    nuc_coord = np.array(nuclei_coord_in_vol[j,:])* np.array([0.018,0.018,0.045])\n",
    "    ais_node_id = get_nearest_skel_node(sk, np.array(ais_coord))\n",
    "    nuc_node_id = get_nearest_skel_node(sk, nuc_coord)\n",
    "    subgraph_axon, subgraph_dend = split_skeleton(G, ais_node_id['id'], nuc_node_id['id'])\n",
    "    axon_nodes = subgraph_axon.nodes\n",
    "    axon_df = sk[sk['id'].isin(axon_nodes)]\n",
    "    axon_len = subgraph_axon.size(weight='weight')\n",
    "    if subgraph_dend == None:\n",
    "        axon_lengths_additional.append(-1)\n",
    "        axon_lengths[idxx] = -1\n",
    "    else:\n",
    "        axon_lengths_additional.append(axon_len)\n",
    "        axon_lengths[idxx] = axon_len\n",
    "    scatter1=go.Scatter3d(x=sk['x'],y=sk['y'],z=sk['z'],mode='markers', marker=dict(size=2))\n",
    "    scatter2=go.Scatter3d(x=axon_df['x'],y=axon_df['y'],z=axon_df['z'],mode='markers', marker=dict(size=2))\n",
    "    scatter_ais = go.Scatter3d(x=[ais_coord[0]], y=[ais_coord[1]], z=[ais_coord[2]], mode='markers', marker=dict(size=5, color='tomato'))\n",
    "    layout = go.Layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z',aspectmode=\"data\"))\n",
    "    fig = go.Figure(data=[scatter1,scatter2, scatter_ais], layout=layout)\n",
    "    pio.write_image(fig, './skel/pyc_with_axon/fig/pyc_'+str(j)+'_'+str(int(axon_len))+'.jpeg', format='jpeg', scale=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdfa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unable_to_rescue = [910,1374,1415,1728,1765,1794,1802]\n",
    "for i in range(0, len(unable_to_rescue)):\n",
    "    j = unable_to_rescue[i]\n",
    "    idxx = pyc_i_with_ais.index(j)\n",
    "    axon_lengths[idxx] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit pandas dataframe\n",
    "\n",
    "print(len(pyc_i_with_ais))\n",
    "print(len(axon_lengths))\n",
    "pc_df['axon_length'] = None\n",
    "pc_df.loc[pyc_i_with_ais, 'axon_length'] = axon_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd0289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional manual annotation AIS (old version)\n",
    "\n",
    "index_added = [510,518,552,564,792,1641,1661,1663,1763,860,883,933,941,955,968,976,982,990]\n",
    "ais_added = [[31808,68874,1474],[55284,55093,1362],[60460,61665,1321],[31088,62532,1424],[57534,50862,923],[50993,41160,917],[39669,44951,1064],[56296,51276,1363],[33606,51896,492],[30601,62568,1161],[59570,54184,1032],[47299,37349,894],[35243,46947,783],[53415,39951,818],[43186,35865,981],[39125,42407,730],[47628,37848,660],[47628,37848,660]]\n",
    "\n",
    "pyc_i_with_ais2 = []\n",
    "ais_all2 = []\n",
    "nuc_coord_all2 = []\n",
    "ais_error2= []\n",
    "\n",
    "for i in range(0, len(index_added)):\n",
    "    pyc_i_with_ais2.append(index_added[i])\n",
    "    pts = ais_added[i]\n",
    "    ais = np.array([pts[0], pts[1], pts[2]]) * np.array([0.018,0.018,0.045])\n",
    "    ais_all2.append(ais)\n",
    "    nuc_coord = np.array(nuclei_coord_in_vol[index_added[i],:])* np.array([0.018,0.018,0.045])\n",
    "    nuc_coord_all2.append(nuc_coord)\n",
    "    try:\n",
    "        skel = pcg_skel.pcg_skeleton(root_id=latest_roots_PC[index_added[i]], client=client, root_point=nuc_coord, collapse_soma=False, collapse_radius=50)\n",
    "        meshparty.skeleton_io.export_to_swc(skel, './skel/pyc_with_axon/pyc_' + str(index_added[i]) + '.swc', node_labels=None, radius=None, \n",
    "                                        header=None, xyz_scaling=1000, resample_spacing=500, interp_kind='linear', \n",
    "                                        tip_length_ratio=0.25, avoid_root=True)\n",
    "        print(i)\n",
    "    except Exception as e:\n",
    "        print('skipping iteration ' + str(i))\n",
    "        continue\n",
    "\n",
    "columns = ['id','type','x','y','z','radius','parent']\n",
    "axon_lengths_additional = []\n",
    "#for i in range(0, 1):\n",
    "for i in range(0, len(index_added)):\n",
    "    j = index_added[i]\n",
    "    sk = pd.read_csv('./skel/pyc_with_axon/pyc_' + str(j) + '.swc', delim_whitespace=True, names=columns)\n",
    "    G = nx.Graph()\n",
    "    for index, row in sk.iterrows():\n",
    "        G.add_node(row['id'], pos=(row['x'], row['y'], row['z']))\n",
    "\n",
    "    for index, row in sk.iterrows():\n",
    "        if row['parent'] != -1:\n",
    "            node1 = sk.loc[sk['id'] == row['id']].iloc[0]\n",
    "            node2 = sk.loc[sk['id'] == row['parent']].iloc[0]\n",
    "            dist = euc_distance(node1, node2)            \n",
    "            G.add_edge(row['id'], row['parent'], weight=dist)\n",
    "    ais_node_id = get_nearest_skel_node(sk, np.array(ais_added[i]) * np.array([0.018,0.018,0.045]))\n",
    "    nuc_node_id = get_nearest_skel_node(sk, nuc_coord_all2[i])\n",
    "    subgraph_axon, subgraph_dend = split_skeleton(G, ais_node_id['id'], nuc_node_id['id'])\n",
    "    axon_nodes = subgraph_axon.nodes\n",
    "    axon_df = sk[sk['id'].isin(axon_nodes)]\n",
    "    axon_len = subgraph_axon.size(weight='weight')\n",
    "    if subgraph_dend == None:\n",
    "        axon_lengths_additional.append(-1)\n",
    "    else:\n",
    "        axon_lengths_additional.append(axon_len)\n",
    "    scatter1=go.Scatter3d(x=sk['x'],y=sk['y'],z=sk['z'],mode='markers', marker=dict(size=2))\n",
    "    scatter2=go.Scatter3d(x=axon_df['x'],y=axon_df['y'],z=axon_df['z'],mode='markers', marker=dict(size=2))\n",
    "    scatter_ais = go.Scatter3d(x=[ais_all2[i][0]], y=[ais_all2[i][1]], z=[ais_all2[i][2]], mode='markers', marker=dict(size=5, color='tomato'))\n",
    "    layout = go.Layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z',aspectmode=\"data\"))\n",
    "    fig = go.Figure(data=[scatter1,scatter2, scatter_ais], layout=layout)\n",
    "    print(i)\n",
    "    pio.write_image(fig, './skel/pyc_with_axon/fig/pyc_'+str(index_added[i])+'_'+str(int(axon_len))+'.jpeg', format='jpeg', scale=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_added = [510,518,552,564,792,1641,1661,1663,1763,860,883,933,941,955,968,976,982,990]\n",
    "\n",
    "print(len(index_added))\n",
    "print(axon_lengths_additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cells with incorrect AIS annotation (i.e. multiply annotated)\n",
    "\n",
    "gc2 = pygsheets.authorize(service_file = \"./hippca3-8126bea0d603.json\")\n",
    "sheet2 = gc2.open('CA3 proofreading')\n",
    "worksheet2 = sheet2.worksheet('title','1.pyramidal_cell_list')\n",
    "pc_df2 = worksheet2.get_as_df(start='A9')\n",
    "pc_df2 = pc_df2.iloc[:-2]\n",
    "\n",
    "nuclei_coord_in_vol = pc_df2[['x','y','z']].to_numpy()\n",
    "latest_roots_PC2 = []\n",
    "print('Getting latest roots for PC...')  \n",
    "cell_ids_scatter = vol.scattered_points(nuclei_coord_in_vol)\n",
    "for i in range(0, len(nuclei_coord_in_vol)):\n",
    "    if i % 300 == 0:\n",
    "        print(i)\n",
    "    cell_ID = [cell_ids_scatter.get(tuple(nuclei_coord_in_vol[i,:]))]\n",
    "    if cell_ID[0] == 0:\n",
    "        cell_ID = np.array([vol[nuclei_coord_in_vol[i,0]-150, nuclei_coord_in_vol[i,1], nuclei_coord_in_vol[i,2]].squeeze()])\n",
    "    if cell_ID[0] == 0:\n",
    "        cell_ID = np.array([vol[nuclei_coord_in_vol[i,0], nuclei_coord_in_vol[i,1]+150, nuclei_coord_in_vol[i,2]].squeeze()])\n",
    "    if cell_ID[0] == 0:\n",
    "        cell_ID = np.array([vol[nuclei_coord_in_vol[i,0], nuclei_coord_in_vol[i,1]-150, nuclei_coord_in_vol[i,2]].squeeze()])\n",
    "    if cell_ID[0] == 0:\n",
    "        print(\"Failed to find neuron segment ID using nuclei center\")\n",
    "    else:\n",
    "        if client.chunkedgraph.is_latest_roots(cell_ID[0]):\n",
    "            latest_roots_PC2.append(cell_ID[0])\n",
    "        else:\n",
    "            latest_roots_PC2.append(client.chunkedgraph.suggest_latest_roots(cell_ID[0]))\n",
    "latest_roots_PC_str2 = [str(num) for num in latest_roots_PC2]\n",
    "\n",
    "worksheet3 = sheet2.worksheet('title','6.Additional_pyramidal')\n",
    "pc_df3 = worksheet3.get_as_df(start='A9')\n",
    "pc_df3 = pc_df3.iloc[:-2]\n",
    "\n",
    "nuclei_coord_in_vol = pc_df3[['x','y','z']].to_numpy()\n",
    "latest_roots_PC3 = []\n",
    "print('Getting latest roots for PC...')  \n",
    "cell_ids_scatter = vol.scattered_points(nuclei_coord_in_vol)\n",
    "for i in range(0, len(nuclei_coord_in_vol)):\n",
    "    if i % 300 == 0:\n",
    "        print(i)\n",
    "    cell_ID = [cell_ids_scatter.get(tuple(nuclei_coord_in_vol[i,:]))]\n",
    "    if cell_ID[0] == 0:\n",
    "        cell_ID = np.array([vol[nuclei_coord_in_vol[i,0]-150, nuclei_coord_in_vol[i,1], nuclei_coord_in_vol[i,2]].squeeze()])\n",
    "    if cell_ID[0] == 0:\n",
    "        cell_ID = np.array([vol[nuclei_coord_in_vol[i,0], nuclei_coord_in_vol[i,1]+150, nuclei_coord_in_vol[i,2]].squeeze()])\n",
    "    if cell_ID[0] == 0:\n",
    "        cell_ID = np.array([vol[nuclei_coord_in_vol[i,0], nuclei_coord_in_vol[i,1]-150, nuclei_coord_in_vol[i,2]].squeeze()])\n",
    "    if cell_ID[0] == 0:\n",
    "        print(\"Failed to find neuron segment ID using nuclei center\")\n",
    "    else:\n",
    "        if client.chunkedgraph.is_latest_roots(cell_ID[0]):\n",
    "            latest_roots_PC3.append(cell_ID[0])\n",
    "        else:\n",
    "            latest_roots_PC3.append(client.chunkedgraph.suggest_latest_roots(cell_ID[0]))\n",
    "latest_roots_PC_str3 = [str(num) for num in latest_roots_PC3]\n",
    "print('---------------')\n",
    "\n",
    "for i in range(0, len(latest_roots_PC2)):\n",
    "    if i not in [293,1269]:\n",
    "        link = pc_df2['Final Link'][i]\n",
    "        time.sleep(0.2)\n",
    "        if link != '':\n",
    "            this_state_id = link.split('/')[-1]\n",
    "            json_info = client.state.get_state_json(state_id =this_state_id)\n",
    "            layerNames = parser.layer_names(json_info)\n",
    "            ais_tab = [i for i, s in enumerate(layerNames) if \"AIS\" in s]\n",
    "            try:\n",
    "                pts, tags = parser.point_annotations(json_info, layerNames[ais_tab[0]], tags=True)\n",
    "            except:\n",
    "                ais_error.append(i+2)\n",
    "                continue\n",
    "            if len(pts) > 1:\n",
    "                print(pc_df2['segid (Nuclei table segID)'].values[i])\n",
    "\n",
    "print('---------------')            \n",
    "\n",
    "for i in range(0, len(latest_roots_PC3)):\n",
    "    link = pc_df3['Final Link'][i]\n",
    "    time.sleep(0.2)\n",
    "    if link != '':\n",
    "        this_state_id = link.split('/')[-1]\n",
    "        json_info = client.state.get_state_json(state_id =this_state_id)\n",
    "        layerNames = parser.layer_names(json_info)\n",
    "        ais_tab = [i for i, s in enumerate(layerNames) if \"AIS\" in s]\n",
    "        try:\n",
    "            pts, tags = parser.point_annotations(json_info, layerNames[ais_tab[0]], tags=True)\n",
    "        except:\n",
    "            ais_error.append(i+2)\n",
    "            continue\n",
    "        if len(pts) > 1:\n",
    "            print(pc_df3['segid (Nuclei table segID)'].values[i])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c158af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk3 = pd.read_csv('./test_skeletor.swc', delim_whitespace=True, names=columns, skiprows=6)\n",
    "sk3[['x','y','z']] = sk3[['x','y','z']] / 1000\n",
    "print(sk3)\n",
    "#axon_nodes = subgraph_axon.nodes\n",
    "#axon_df = sk[sk['id'].isin(axon_nodes)]\n",
    "scatter11=go.Scatter3d(x=sk3['x'],y=sk3['y'],z=sk3['z'],mode='markers', marker=dict(size=2))\n",
    "#scatter12=go.Scatter3d(x=axon_df['x'],y=axon_df['y'],z=axon_df['z'],mode='markers', marker=dict(size=2))\n",
    "scatter_ais = go.Scatter3d(x=[ais_all[0][0]], y=[ais_all[0][1]], z=[ais_all[0][2]], mode='markers', marker=dict(size=10, color='tomato'))\n",
    "scatter_nuc = go.Scatter3d(x=[nuc_coord_all[0][0]], y=[nuc_coord_all[0][1]], z=[nuc_coord_all[0][2]], mode='markers', marker=dict(size=20, color='violet'))\n",
    "layout = go.Layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z',aspectmode=\"data\"))\n",
    "fig = go.Figure(data=[scatter11, scatter_ais, scatter_nuc], layout=layout)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
